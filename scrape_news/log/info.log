2017-12-14 13:25:55,014 - loggers - INFO -  Starting crawl SinaSpider
2017-12-14 13:25:56,361 - loggers - INFO - this is a test
2017-12-14 13:25:56,362 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 13:25:56,362 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 13:25:56,405 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 13:25:56,634 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 13:25:56,637 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 13:25:56,767 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 13:25:56,767 - scrapy.core.engine - INFO - Spider opened
2017-12-14 13:25:56,850 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 13:26:05,473 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7197127.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 43, in parse_detail
    item[PUBLISHTIME] = response.xpath(xpath.get_addtime()).extract_first().strip()
NameError: name 'PUBLISHTIME' is not defined
2017-12-14 13:26:09,442 - scrapy.crawler - INFO - Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-12-14 13:26:09,443 - scrapy.core.engine - INFO - Closing spider (shutdown)
2017-12-14 13:26:09,922 - scrapy.crawler - INFO - Received SIG_UNBLOCK twice, forcing unclean shutdown
2017-12-14 13:26:10,488 - loggers - INFO -  Starting crawl QqSpider
2017-12-14 13:26:11,358 - loggers - INFO -  Starting crawl ChinanbaSpider
2017-12-14 13:26:11,546 - loggers - INFO -  Starting crawl ZhibobaSpider
2017-12-14 13:34:58,441 - loggers - INFO - this is a test
2017-12-14 13:34:58,443 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 13:34:58,443 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 13:34:58,488 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 13:34:58,734 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 13:34:58,738 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 13:34:58,863 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 13:34:58,864 - scrapy.core.engine - INFO - Spider opened
2017-12-14 13:34:58,960 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 13:35:00,444 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 13:35:00,445 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/request_bytes': 220,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 307,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 5, 35, 0, 444648),
 'log_count/DEBUG': 4,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 12, 14, 5, 34, 58, 960392)}
2017-12-14 13:35:00,445 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 13:35:08,754 - loggers - INFO - this is a test
2017-12-14 13:35:08,754 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 13:35:08,755 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 13:35:08,779 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 13:35:08,920 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 13:35:08,922 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 13:35:09,022 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 13:35:09,022 - scrapy.core.engine - INFO - Spider opened
2017-12-14 13:35:09,094 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 13:35:13,539 - scrapy.core.scraper - ERROR - Error processing {'addtime': '2017-12-14 13:35:13',
 'content': '\u3000\u3000'
            '北京时间12月14日，火箭在主场以108-96战胜黄蜂。上半场快结束时，火箭替补前锋巴莫特在完成一次扣篮之后不幸肩膀受伤，只能提前退场。\u3000\u3000'
            '当时比赛进行到第二节还剩1分多钟，巴莫特突破之后完成了一个双手扣篮，非常精彩。但是在从篮框上落地的过程中，巴莫特明显重心不稳，结果他的右手撑了一下地板，然后巴莫特将倒在地板上，表情痛苦，而且一直用左手握着右手臂。\u3000\u3000'
            '那段时间，巴莫特迟迟没能起身，而火箭队训练师琼斯一直在巴莫特身边查看他的情况，现场甚至把担架车也抬了上来。\u3000\u3000'
            '当巴莫特站起来以后，他依然面带微笑，而现场的火箭球迷一起给予巴莫特掌声和鼓励。巴莫特没有选择坐担架车，而是在琼斯的搀扶下慢慢走回更衣室。\u3000\u3000'
            '中场休息时，火箭队宣布巴莫特因为肩膀受伤，本场比赛不会回来。\u3000\u3000'
            '本场比赛，巴莫特替补出场9分钟，2投1中，得到2分、1个篮板和1次助攻。\u3000\u3000'
            '本赛季，巴莫特是火箭队很重要的一名轮换球员，特别是他的防守能力，希望他这次受伤不是很重。\u3000\u3000（罗森）',
 'cover': 'http://n.sinaimg.cn/sports/transform/w498h366/20171214/d8Yg-fypsqiz7209754.jpg',
 'link': 'http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7210122.shtml',
 'publishtime': '2-0-1 7:年',
 'source': '新浪体育',
 'title': '火箭大胜蒙上一层阴影 强援暴扣却把自己伤了'}
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/vincent/pyProjects/scrape/scrape_news/pipelines.py", line 46, in process_item
    if redis.sadd(redis.get_key_name(), item['link']) == 1:
  File "/Users/vincent/pyProjects/scrape/utils/redisDb.py", line 12, in sadd
    return self.redis_db.sadd(col_name, value)
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/client.py", line 1600, in sadd
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/client.py", line 667, in execute_command
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/connection.py", line 610, in send_command
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/connection.py", line 585, in send_packed_command
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/connection.py", line 493, in connect
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/connection.py", line 561, in on_connect
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/connection.py", line 629, in read_response
redis.exceptions.ResponseError: invalid password
2017-12-14 13:35:17,346 - scrapy.core.scraper - ERROR - Error processing {'addtime': '2017-12-14 13:35:17',
 'content': '\u3000\u3000'
            '北京时间12月14日，黄蜂在客场以96-108负于火箭。黄蜂首发中锋德怀特-霍华德出场34分钟，20投9中，罚球11中8，得到全队最高的26分和全场最高的18个篮板，外加3次盖帽。\u3000\u3000'
            '霍华德今天是回到休斯顿来对阵老东家。很显然，霍华德离开火箭队的方式并不愉快，所以每次来休斯顿打火箭，他总是渴望能够证明自己。\u3000\u3000'
            '本场比赛开始后不久，霍华德就来了一次快攻表演，当时他自己运球发动快攻，尽管有些踉跄，而且差点被保罗断球，但是霍华德依靠身高优势拿回球权，并完成一个快攻扣篮。\u3000\u3000'
            '防守端，魔兽在篮下依然是很强的存在。火箭队小前锋阿里扎在一次突破中试图在魔兽面前上篮，结果被扇了大帽。\u3000\u3000'
            '由于火箭队的进攻能力太强，等到霍华德在第二节重新登场时，黄蜂队已经落后20多分。这一节，霍华德在篮下展现出了统治力，他在低位连续凿了卡培拉2个球，而且小勾手练得很熟练。\u3000\u3000'
            '更难得的是，霍华德的罚球提高了不少，本节最后1分钟，霍华德4罚全中，而黄蜂在半场结束时将落后分差缩小到14分。\u3000\u3000'
            '上半场，霍华德10投6中，9罚6中，得到18分和10个篮板，都是全队最高。\u3000\u3000'
            '下半场开始后，霍华德依然是硬凿卡培拉。第三节还剩8分钟，霍华德低位背打卡培拉，并完成一个左手的抛投；下一个回合，霍华德在篮下抢到进攻篮板，并重扣得分。\u3000\u3000'
            '尽管黄蜂输给了火箭，但是霍华德今天完爆卡培拉，他20投9中，罚球11中8，得到全队最高的26分和全场最高的18个篮板，外加3次盖帽。\u3000\u3000'
            '（罗森）',
 'cover': 'http://n.sinaimg.cn/sports/transform/w650h433/20171214/O7zw-fypsqiz7221582.jpg',
 'link': 'http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7222235.shtml',
 'publishtime': '2-0-1 7:年',
 'source': '新浪体育',
 'title': '26+18+3帽！魔兽跟火箭多大仇 球队输了他没输'}
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/vincent/pyProjects/scrape/scrape_news/pipelines.py", line 46, in process_item
    if redis.sadd(redis.get_key_name(), item['link']) == 1:
  File "/Users/vincent/pyProjects/scrape/utils/redisDb.py", line 12, in sadd
    return self.redis_db.sadd(col_name, value)
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/client.py", line 1600, in sadd
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/client.py", line 667, in execute_command
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/connection.py", line 610, in send_command
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/connection.py", line 585, in send_packed_command
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/connection.py", line 493, in connect
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/connection.py", line 561, in on_connect
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/connection.py", line 629, in read_response
redis.exceptions.ResponseError: invalid password
2017-12-14 13:35:21,217 - scrapy.core.scraper - ERROR - Error processing {'addtime': '2017-12-14 13:35:21',
 'content': '\u3000\u3000'
            '北京时间12月14日，火箭在主场以108-96击败黄蜂，取得11连胜，并追平了队史第4长连胜。\u3000\u3000'
            '火箭队历史上的前3场连胜依次是：22连胜、15连胜（2次）和13连胜。\u3000\u3000'
            '今天火箭队主场击败黄蜂队之后，火箭已经取得11连胜，追平了他们在1991年4月的11连胜。\u3000\u3000'
            '但值得一提的是，自从火箭队在2008年取得22连胜之后，11连胜已经是火箭队取得过的最长连胜。去年12月，火箭队曾打出过一波10连胜。\u3000\u3000'
            '就本赛季来说，11连胜可以在联盟里排名第3，仅次于凯尔特人的16连胜和骑士的13连胜。以火箭目前的状态，他们还可以把这波连胜持续得更长。\u3000\u3000'
            '（罗森）',
 'cover': 'http://n.sinaimg.cn/sports/transform/w513h458/20171214/jlSD-fypsqiz7235706.jpg',
 'link': 'http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7236362.shtml',
 'publishtime': '2-0-1 7:年',
 'source': '新浪体育',
 'title': '11连胜！火箭一骑绝尘 这波是22连胜后最强'}
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/vincent/pyProjects/scrape/scrape_news/pipelines.py", line 46, in process_item
    if redis.sadd(redis.get_key_name(), item['link']) == 1:
  File "/Users/vincent/pyProjects/scrape/utils/redisDb.py", line 12, in sadd
    return self.redis_db.sadd(col_name, value)
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/client.py", line 1600, in sadd
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/client.py", line 667, in execute_command
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/connection.py", line 610, in send_command
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/connection.py", line 585, in send_packed_command
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/connection.py", line 493, in connect
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/connection.py", line 561, in on_connect
  File "/anaconda3/lib/python3.6/site-packages/redis-2.10.6-py3.6.egg/redis/connection.py", line 629, in read_response
redis.exceptions.ResponseError: invalid password
2017-12-14 13:35:22,032 - scrapy.crawler - INFO - Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-12-14 13:35:22,033 - scrapy.core.engine - INFO - Closing spider (shutdown)
2017-12-14 13:35:22,480 - scrapy.crawler - INFO - Received SIG_UNBLOCK twice, forcing unclean shutdown
2017-12-14 13:36:39,311 - loggers - INFO - this is a test
2017-12-14 13:36:39,312 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 13:36:39,312 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 13:36:39,336 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 13:36:39,471 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 13:36:39,473 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 13:36:39,577 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 13:36:39,578 - scrapy.core.engine - INFO - Spider opened
2017-12-14 13:36:40,410 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 13:37:01,756 - scrapy.crawler - INFO - Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-12-14 13:37:01,762 - scrapy.core.engine - INFO - Closing spider (shutdown)
2017-12-14 13:37:01,992 - scrapy.crawler - INFO - Received SIG_UNBLOCK twice, forcing unclean shutdown
2017-12-14 13:45:20,688 - loggers - INFO - this is a test
2017-12-14 13:45:20,689 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 13:45:20,690 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 13:45:20,729 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 13:45:20,731 - twisted - CRITICAL - Unhandled error in Deferred:
2017-12-14 13:45:20,732 - twisted - CRITICAL - 
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 96, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/SinaSpider.py", line 13, in __init__
    super().__init__(config=self.config)
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 17, in __init__
    self.data = json2Dict(config_path)
  File "/Users/vincent/pyProjects/scrape/utils/json2Dict.py", line 48, in __init__
    self.xpath = xpath(json_data[XPATH])
  File "/Users/vincent/pyProjects/scrape/utils/json2Dict.py", line 9, in __init__
    self.publishtime = json_data[PUBLISHTIME]
KeyError: 'publishtime'
2017-12-14 13:46:10,787 - loggers - INFO - this is a test
2017-12-14 13:46:10,788 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 13:46:10,788 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 13:46:10,809 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 13:46:11,085 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 13:46:11,089 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 13:46:11,210 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 13:46:11,215 - scrapy.core.engine - INFO - Spider opened
2017-12-14 13:46:11,286 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 13:46:54,782 - scrapy.crawler - INFO - Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-12-14 13:46:54,782 - scrapy.core.engine - INFO - Closing spider (shutdown)
2017-12-14 13:46:55,030 - scrapy.crawler - INFO - Received SIG_UNBLOCK twice, forcing unclean shutdown
2017-12-14 13:47:02,705 - loggers - INFO - this is a test
2017-12-14 13:47:02,706 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 13:47:02,706 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 13:47:02,751 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 13:47:02,995 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 13:47:02,999 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 13:47:03,119 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 13:47:03,119 - scrapy.core.engine - INFO - Spider opened
2017-12-14 13:47:03,191 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 13:47:10,119 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.qq.com/nba/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 13:47:10,226 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 13:47:10,228 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 645,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 5, 47, 10, 227885),
 'log_count/DEBUG': 10,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 12, 14, 5, 47, 3, 191205)}
2017-12-14 13:47:10,228 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 13:48:02,346 - loggers - INFO - this is a test
2017-12-14 13:48:02,346 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 13:48:02,347 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 13:48:02,370 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 13:48:02,504 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 13:48:02,506 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 13:48:02,608 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 13:48:02,608 - scrapy.core.engine - INFO - Spider opened
2017-12-14 13:48:02,675 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 13:49:02,679 - scrapy.extensions.logstats - INFO - Crawled 9 pages (at 9 pages/min), scraped 8 items (at 8 items/min)
2017-12-14 13:50:02,680 - scrapy.extensions.logstats - INFO - Crawled 16 pages (at 7 pages/min), scraped 15 items (at 7 items/min)
2017-12-14 13:50:42,456 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 13:50:42,457 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 3,
 'downloader/request_bytes': 6111,
 'downloader/request_count': 23,
 'downloader/request_method_count/GET': 23,
 'downloader/response_bytes': 716651,
 'downloader/response_count': 20,
 'downloader/response_status_count/200': 19,
 'downloader/response_status_count/500': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 5, 50, 42, 456948),
 'item_scraped_count': 18,
 'log_count/DEBUG': 88,
 'log_count/INFO': 9,
 'request_depth_max': 1,
 'response_received_count': 19,
 'scheduler/dequeued': 23,
 'scheduler/dequeued/memory': 23,
 'scheduler/enqueued': 23,
 'scheduler/enqueued/memory': 23,
 'start_time': datetime.datetime(2017, 12, 14, 5, 48, 2, 675502)}
2017-12-14 13:50:42,457 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 14:02:41,767 - loggers - INFO - this is a test
2017-12-14 14:02:41,768 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 14:02:41,768 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 14:02:41,813 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 14:02:42,097 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 14:02:42,101 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 14:02:42,235 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 14:02:42,235 - scrapy.core.engine - INFO - Spider opened
2017-12-14 14:02:42,318 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 14:02:59,937 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/002191.htm
2017-12-14 14:03:09,298 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/005549.htm
2017-12-14 14:03:20,090 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/000868.htm
2017-12-14 14:03:21,209 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/011817.htm
2017-12-14 14:03:36,015 - root - INFO - 已经爬过http://sports.qq.com/a/20170302/010011.htm
2017-12-14 14:03:42,350 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/012005.htm
2017-12-14 14:03:42,357 - scrapy.extensions.logstats - INFO - Crawled 7 pages (at 7 pages/min), scraped 6 items (at 6 items/min)
2017-12-14 14:03:51,798 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/016941.htm
2017-12-14 14:03:52,993 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/003027.htm
2017-12-14 14:04:09,090 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/001106.htm
2017-12-14 14:04:11,682 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/017053.htm
2017-12-14 14:04:12,693 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/005702.htm
2017-12-14 14:04:15,663 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/009426.htm
2017-12-14 14:04:18,094 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/016269.htm
2017-12-14 14:04:21,327 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/003278.htm
2017-12-14 14:04:42,318 - scrapy.extensions.logstats - INFO - Crawled 15 pages (at 8 pages/min), scraped 14 items (at 8 items/min)
2017-12-14 14:05:04,640 - root - INFO - 已经爬过http://sports.qq.com/a/20161019/029134.htm
2017-12-14 14:05:32,318 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/016319.htm
2017-12-14 14:05:35,900 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/002486.htm
2017-12-14 14:05:42,321 - scrapy.extensions.logstats - INFO - Crawled 18 pages (at 3 pages/min), scraped 17 items (at 3 items/min)
2017-12-14 14:06:14,444 - root - INFO - 已经爬过http://sports.qq.com/a/20171003/007728.htm
2017-12-14 14:06:14,447 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 14:06:14,449 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 6,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 2,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 4,
 'downloader/request_bytes': 6647,
 'downloader/request_count': 25,
 'downloader/request_method_count/GET': 25,
 'downloader/response_bytes': 915178,
 'downloader/response_count': 19,
 'downloader/response_status_count/200': 19,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 6, 6, 14, 448771),
 'item_scraped_count': 18,
 'log_count/DEBUG': 94,
 'log_count/INFO': 28,
 'request_depth_max': 1,
 'response_received_count': 19,
 'scheduler/dequeued': 25,
 'scheduler/dequeued/memory': 25,
 'scheduler/enqueued': 25,
 'scheduler/enqueued/memory': 25,
 'start_time': datetime.datetime(2017, 12, 14, 6, 2, 42, 318042)}
2017-12-14 14:06:14,461 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 15:05:08,990 - loggers - INFO -  Starting crawl SinaSpider
2017-12-14 15:05:10,455 - loggers - INFO - this is a test
2017-12-14 15:05:10,456 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 15:05:10,456 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 15:05:10,502 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 15:05:10,763 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 15:05:10,770 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 15:05:10,898 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 15:05:10,898 - scrapy.core.engine - INFO - Spider opened
2017-12-14 15:05:10,975 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 15:06:10,979 - scrapy.extensions.logstats - INFO - Crawled 13 pages (at 13 pages/min), scraped 12 items (at 12 items/min)
2017-12-14 15:07:10,979 - scrapy.extensions.logstats - INFO - Crawled 28 pages (at 15 pages/min), scraped 27 items (at 15 items/min)
2017-12-14 15:07:25,556 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 15:07:25,557 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 9460,
 'downloader/request_count': 31,
 'downloader/request_method_count/GET': 31,
 'downloader/response_bytes': 952596,
 'downloader/response_count': 30,
 'downloader/response_status_count/200': 29,
 'downloader/response_status_count/500': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 7, 7, 25, 557236),
 'item_scraped_count': 28,
 'log_count/DEBUG': 122,
 'log_count/INFO': 9,
 'request_depth_max': 1,
 'response_received_count': 29,
 'scheduler/dequeued': 31,
 'scheduler/dequeued/memory': 31,
 'scheduler/enqueued': 31,
 'scheduler/enqueued/memory': 31,
 'start_time': datetime.datetime(2017, 12, 14, 7, 5, 10, 975525)}
2017-12-14 15:07:25,557 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 15:07:25,842 - loggers - INFO -  Starting crawl QqSpider
2017-12-14 15:07:27,151 - loggers - INFO - this is a test
2017-12-14 15:07:27,151 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 15:07:27,152 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 15:07:27,190 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 15:07:27,425 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 15:07:27,431 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 15:07:27,554 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 15:07:27,554 - scrapy.core.engine - INFO - Spider opened
2017-12-14 15:07:27,626 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 15:07:38,213 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/016941.htm
2017-12-14 15:07:41,680 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/002486.htm
2017-12-14 15:07:46,680 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/001106.htm
2017-12-14 15:07:48,890 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/009426.htm
2017-12-14 15:07:58,987 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/005549.htm
2017-12-14 15:08:01,521 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/002191.htm
2017-12-14 15:08:06,505 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/003027.htm
2017-12-14 15:08:10,671 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/016319.htm
2017-12-14 15:08:13,602 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/003278.htm
2017-12-14 15:08:13,777 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/000868.htm
2017-12-14 15:08:16,609 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/011817.htm
2017-12-14 15:08:21,090 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/017053.htm
2017-12-14 15:08:24,005 - root - INFO - 已经爬过http://sports.qq.com/a/20170302/010011.htm
2017-12-14 15:08:27,627 - scrapy.extensions.logstats - INFO - Crawled 14 pages (at 14 pages/min), scraped 13 items (at 13 items/min)
2017-12-14 15:08:32,573 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/012005.htm
2017-12-14 15:08:35,323 - root - INFO - 已经爬过http://sports.qq.com/a/20171003/007728.htm
2017-12-14 15:08:37,292 - root - INFO - 已经爬过http://sports.qq.com/a/20161019/029134.htm
2017-12-14 15:08:49,876 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/016269.htm
2017-12-14 15:08:49,879 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 15:08:49,880 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/request_bytes': 5320,
 'downloader/request_count': 20,
 'downloader/request_method_count/GET': 20,
 'downloader/response_bytes': 716722,
 'downloader/response_count': 20,
 'downloader/response_status_count/200': 19,
 'downloader/response_status_count/302': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 7, 8, 49, 879950),
 'item_scraped_count': 18,
 'log_count/DEBUG': 79,
 'log_count/INFO': 25,
 'request_depth_max': 1,
 'response_received_count': 19,
 'scheduler/dequeued': 20,
 'scheduler/dequeued/memory': 20,
 'scheduler/enqueued': 20,
 'scheduler/enqueued/memory': 20,
 'start_time': datetime.datetime(2017, 12, 14, 7, 7, 27, 626699)}
2017-12-14 15:08:49,880 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 15:08:50,228 - loggers - INFO -  Starting crawl ChinanbaSpider
2017-12-14 15:08:51,498 - loggers - INFO - this is a test
2017-12-14 15:08:51,499 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 15:08:51,499 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 15:08:51,538 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 15:08:51,751 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 15:08:51,756 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 15:08:51,877 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 15:08:51,877 - scrapy.core.engine - INFO - Spider opened
2017-12-14 15:08:51,954 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 15:09:51,956 - scrapy.extensions.logstats - INFO - Crawled 14 pages (at 14 pages/min), scraped 13 items (at 13 items/min)
2017-12-14 15:10:51,955 - scrapy.extensions.logstats - INFO - Crawled 23 pages (at 9 pages/min), scraped 22 items (at 9 items/min)
2017-12-14 15:11:51,958 - scrapy.extensions.logstats - INFO - Crawled 23 pages (at 0 pages/min), scraped 22 items (at 0 items/min)
2017-12-14 15:12:51,955 - scrapy.extensions.logstats - INFO - Crawled 24 pages (at 1 pages/min), scraped 23 items (at 1 items/min)
2017-12-14 15:13:17,448 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 15:13:17,449 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 4,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 2,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 7804,
 'downloader/request_count': 29,
 'downloader/request_method_count/GET': 29,
 'downloader/response_bytes': 687822,
 'downloader/response_count': 25,
 'downloader/response_status_count/200': 25,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 7, 13, 17, 448821),
 'item_scraped_count': 24,
 'log_count/DEBUG': 112,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 25,
 'scheduler/dequeued': 29,
 'scheduler/dequeued/memory': 29,
 'scheduler/enqueued': 29,
 'scheduler/enqueued/memory': 29,
 'start_time': datetime.datetime(2017, 12, 14, 7, 8, 51, 954309)}
2017-12-14 15:13:17,454 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 15:13:17,818 - loggers - INFO -  Starting crawl ZhibobaSpider
2017-12-14 15:13:19,234 - loggers - INFO - this is a test
2017-12-14 15:13:19,235 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 15:13:19,235 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 15:13:19,279 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 15:13:19,523 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 15:13:19,527 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 15:13:19,649 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 15:13:19,649 - scrapy.core.engine - INFO - Spider opened
2017-12-14 15:13:19,723 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 15:13:27,199 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'unknown protocol')]>]
2017-12-14 15:13:27,306 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 15:13:27,307 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/scrapy.core.downloader.handlers.http11.TunnelError': 2,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 648,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 7, 13, 27, 307051),
 'log_count/DEBUG': 10,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 12, 14, 7, 13, 19, 722936)}
2017-12-14 15:13:27,307 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 15:21:52,498 - loggers - INFO - this is a test
2017-12-14 15:21:52,499 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 15:21:52,499 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 15:21:52,541 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 15:21:52,773 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 15:21:52,778 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 15:21:52,905 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 15:21:52,906 - scrapy.core.engine - INFO - Spider opened
2017-12-14 15:21:52,985 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 15:22:52,987 - scrapy.extensions.logstats - INFO - Crawled 5 pages (at 5 pages/min), scraped 4 items (at 4 items/min)
2017-12-14 15:23:52,987 - scrapy.extensions.logstats - INFO - Crawled 14 pages (at 9 pages/min), scraped 13 items (at 9 items/min)
2017-12-14 15:24:52,989 - scrapy.extensions.logstats - INFO - Crawled 22 pages (at 8 pages/min), scraped 21 items (at 8 items/min)
2017-12-14 15:25:08,063 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2017-12-14/5a31e033b3504.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
scrapy.core.downloader.handlers.http11.TunnelError: Could not open CONNECT tunnel with proxy 114.215.103.121:8081 [{'status': 400, 'reason': b'Bad Request'}]
2017-12-14 15:25:13,265 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2017-12-14/106662.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
scrapy.core.downloader.handlers.http11.TunnelError: Could not open CONNECT tunnel with proxy 118.193.107.210:80 [{'status': 400, 'reason': b'Bad Request'}]
2017-12-14 15:25:20,617 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2017-12-14/5a31eff626dbb.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
scrapy.core.downloader.handlers.http11.TunnelError: Could not open CONNECT tunnel with proxy 118.193.107.62:80 [{'status': 400, 'reason': b'Bad Request'}]
2017-12-14 15:25:28,583 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2017-12-14/5a31b571e0b27.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
scrapy.core.downloader.handlers.http11.TunnelError: Could not open CONNECT tunnel with proxy 58.56.128.84:9001 [{'status': 500, 'reason': b'Internal Server Error'}]
2017-12-14 15:25:52,990 - scrapy.extensions.logstats - INFO - Crawled 27 pages (at 5 pages/min), scraped 26 items (at 5 items/min)
2017-12-14 15:26:00,572 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2017-12-13/5a31013a8add0.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
scrapy.core.downloader.handlers.http11.TunnelError: Could not open CONNECT tunnel with proxy 123.57.76.102:80 [{'status': 500, 'reason': b'Internal Server Error'}]
2017-12-14 15:26:10,913 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2017-12-14/106661.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2017-12-14 15:26:52,990 - scrapy.extensions.logstats - INFO - Crawled 27 pages (at 0 pages/min), scraped 26 items (at 0 items/min)
2017-12-14 15:27:52,989 - scrapy.extensions.logstats - INFO - Crawled 27 pages (at 0 pages/min), scraped 26 items (at 0 items/min)
2017-12-14 15:28:07,081 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 15:28:07,088 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 35,
 'downloader/exception_type_count/scrapy.core.downloader.handlers.http11.TunnelError': 32,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 17574,
 'downloader/request_count': 63,
 'downloader/request_method_count/GET': 63,
 'downloader/response_bytes': 207149,
 'downloader/response_count': 28,
 'downloader/response_status_count/200': 28,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 7, 28, 7, 87755),
 'item_scraped_count': 27,
 'log_count/DEBUG': 217,
 'log_count/ERROR': 6,
 'log_count/INFO': 13,
 'request_depth_max': 1,
 'response_received_count': 28,
 'scheduler/dequeued': 63,
 'scheduler/dequeued/memory': 63,
 'scheduler/enqueued': 63,
 'scheduler/enqueued/memory': 63,
 'start_time': datetime.datetime(2017, 12, 14, 7, 21, 52, 985125)}
2017-12-14 15:28:07,089 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 16:16:04,172 - loggers - INFO - this is a test
2017-12-14 16:16:04,173 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 16:16:04,174 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 16:16:04,218 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 16:16:04,451 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 16:16:04,455 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 16:16:04,577 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 16:16:04,577 - scrapy.core.engine - INFO - Spider opened
2017-12-14 16:16:04,651 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 16:16:19,619 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a31a575086d9.htm
2017-12-14 16:16:21,019 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a31ad9e22fb6.htm
2017-12-14 16:16:27,217 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a31c4f0c032a.htm
2017-12-14 16:16:34,308 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a320de974f69.htm
2017-12-14 16:17:02,931 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/106664.htm
2017-12-14 16:17:04,654 - scrapy.extensions.logstats - INFO - Crawled 9 pages (at 9 pages/min), scraped 8 items (at 8 items/min)
2017-12-14 16:17:06,023 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a31eb38c5c4e.htm
2017-12-14 16:17:09,840 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a31fb3e0eb18.htm
2017-12-14 16:17:12,608 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a31f65ca125d.htm
2017-12-14 16:17:16,871 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/106666.htm
2017-12-14 16:17:26,043 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a3208cbaf129.htm
2017-12-14 16:17:30,626 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a320c02cf3bf.htm
2017-12-14 16:17:42,586 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/106663.htm
2017-12-14 16:17:45,234 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/106665.htm
2017-12-14 16:18:04,651 - scrapy.extensions.logstats - INFO - Crawled 18 pages (at 9 pages/min), scraped 16 items (at 8 items/min)
2017-12-14 16:18:04,706 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a320563177eb.htm
2017-12-14 16:18:25,782 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a3186e2a8ba3.htm
2017-12-14 16:18:43,483 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a31b4fcab882.htm
2017-12-14 16:19:04,653 - scrapy.extensions.logstats - INFO - Crawled 23 pages (at 5 pages/min), scraped 22 items (at 6 items/min)
2017-12-14 16:19:05,590 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/106667.htm
2017-12-14 16:19:10,241 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2017-12-13/5a312a9360ee4.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
scrapy.core.downloader.handlers.http11.TunnelError: Could not open CONNECT tunnel with proxy 114.215.103.121:8081 [{'status': 400, 'reason': b'Bad Request'}]
2017-12-14 16:19:10,692 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a3206033797d.htm
2017-12-14 16:19:16,905 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a31fedd29a6c.htm
2017-12-14 16:19:17,986 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2017-12-14/5a31e58a46067.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
scrapy.core.downloader.handlers.http11.TunnelError: Could not open CONNECT tunnel with proxy 118.193.107.186:80 [{'status': 400, 'reason': b'Bad Request'}]
2017-12-14 16:19:26,863 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a31f848a04f4.htm
2017-12-14 16:19:27,896 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a31f124b2aed.htm
2017-12-14 16:19:28,436 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a31a79fc3619.htm
2017-12-14 16:19:34,199 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a31f254e97bf.htm
2017-12-14 16:19:37,778 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/106659.htm
2017-12-14 16:20:04,652 - scrapy.extensions.logstats - INFO - Crawled 31 pages (at 8 pages/min), scraped 30 items (at 8 items/min)
2017-12-14 16:21:04,655 - scrapy.extensions.logstats - INFO - Crawled 31 pages (at 0 pages/min), scraped 30 items (at 0 items/min)
2017-12-14 16:21:35,353 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 16:21:35,360 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 27,
 'downloader/exception_type_count/scrapy.core.downloader.handlers.http11.TunnelError': 22,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 4,
 'downloader/request_bytes': 16488,
 'downloader/request_count': 59,
 'downloader/request_method_count/GET': 59,
 'downloader/response_bytes': 230659,
 'downloader/response_count': 32,
 'downloader/response_status_count/200': 32,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 8, 21, 35, 359634),
 'item_scraped_count': 31,
 'log_count/DEBUG': 209,
 'log_count/ERROR': 2,
 'log_count/INFO': 36,
 'request_depth_max': 1,
 'response_received_count': 32,
 'scheduler/dequeued': 59,
 'scheduler/dequeued/memory': 59,
 'scheduler/enqueued': 59,
 'scheduler/enqueued/memory': 59,
 'start_time': datetime.datetime(2017, 12, 14, 8, 16, 4, 651440)}
2017-12-14 16:21:35,361 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 16:42:19,006 - loggers - INFO - this is a test
2017-12-14 16:42:19,007 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 16:42:19,008 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 16:42:19,052 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 16:42:19,298 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 16:42:19,302 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 16:42:19,443 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 16:42:19,443 - scrapy.core.engine - INFO - Spider opened
2017-12-14 16:42:19,530 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 16:42:29,060 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/016269.htm
2017-12-14 16:42:31,201 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/016319.htm
2017-12-14 16:42:34,606 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/000868.htm
2017-12-14 16:42:38,093 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/005549.htm
2017-12-14 16:42:43,010 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/011817.htm
2017-12-14 16:42:46,801 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/017053.htm
2017-12-14 16:42:57,968 - root - INFO - 已经爬过http://sports.qq.com/a/20161019/029134.htm
2017-12-14 16:42:59,051 - root - INFO - 已经爬过http://sports.qq.com/a/20170302/010011.htm
2017-12-14 16:43:01,967 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/009426.htm
2017-12-14 16:43:02,331 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/012005.htm
2017-12-14 16:43:02,880 - root - INFO - 已经爬过http://sports.qq.com/a/20171003/007728.htm
2017-12-14 16:43:05,785 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/016941.htm
2017-12-14 16:43:08,807 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/003027.htm
2017-12-14 16:43:11,939 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/002486.htm
2017-12-14 16:43:15,303 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/001106.htm
2017-12-14 16:43:19,533 - scrapy.extensions.logstats - INFO - Crawled 17 pages (at 17 pages/min), scraped 16 items (at 16 items/min)
2017-12-14 16:43:19,763 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/002191.htm
2017-12-14 16:43:21,773 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/003278.htm
2017-12-14 16:43:21,780 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 16:43:21,781 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/request_bytes': 5039,
 'downloader/request_count': 19,
 'downloader/request_method_count/GET': 19,
 'downloader/response_bytes': 716474,
 'downloader/response_count': 19,
 'downloader/response_status_count/200': 19,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 8, 43, 21, 780912),
 'item_scraped_count': 18,
 'log_count/DEBUG': 76,
 'log_count/INFO': 25,
 'request_depth_max': 1,
 'response_received_count': 19,
 'scheduler/dequeued': 19,
 'scheduler/dequeued/memory': 19,
 'scheduler/enqueued': 19,
 'scheduler/enqueued/memory': 19,
 'start_time': datetime.datetime(2017, 12, 14, 8, 42, 19, 529804)}
2017-12-14 16:43:21,781 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 16:45:36,517 - loggers - INFO -  Starting crawl SinaSpider
2017-12-14 16:45:38,037 - loggers - INFO - this is a test
2017-12-14 16:45:38,038 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 16:45:38,038 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 16:45:38,075 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 16:45:38,283 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 16:45:38,289 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 16:45:38,416 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 16:45:38,416 - scrapy.core.engine - INFO - Spider opened
2017-12-14 16:45:38,489 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 16:46:14,036 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7197127.shtml
2017-12-14 16:46:29,780 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6249797.shtml
2017-12-14 16:46:37,117 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4436015.shtml
2017-12-14 16:46:38,492 - scrapy.extensions.logstats - INFO - Crawled 4 pages (at 4 pages/min), scraped 3 items (at 3 items/min)
2017-12-14 16:46:45,362 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4438577.shtml
2017-12-14 16:46:50,220 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7144560.shtml
2017-12-14 16:46:53,870 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4407128.shtml
2017-12-14 16:46:59,599 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7201812.shtml
2017-12-14 16:47:02,434 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4455648.shtml
2017-12-14 16:47:06,225 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4460600.shtml
2017-12-14 16:47:11,403 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7149014.shtml
2017-12-14 16:47:11,447 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4460742.shtml
2017-12-14 16:47:12,963 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7160180.shtml
2017-12-14 16:47:15,144 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4461518.shtml
2017-12-14 16:47:24,768 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4431588.shtml
2017-12-14 16:47:26,317 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4461895.shtml
2017-12-14 16:47:26,470 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6251370.shtml
2017-12-14 16:47:30,115 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7278660.shtml
2017-12-14 16:47:30,848 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4479877.shtml
2017-12-14 16:47:32,909 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7315698.shtml
2017-12-14 16:47:36,220 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7364695.shtml
2017-12-14 16:47:38,489 - scrapy.extensions.logstats - INFO - Crawled 21 pages (at 17 pages/min), scraped 20 items (at 17 items/min)
2017-12-14 16:47:52,611 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7388627.shtml
2017-12-14 16:47:56,883 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6247792.shtml
2017-12-14 16:48:01,816 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4406922.shtml
2017-12-14 16:48:03,304 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6254501.shtml
2017-12-14 16:48:10,557 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7210740.shtml
2017-12-14 16:48:38,493 - scrapy.extensions.logstats - INFO - Crawled 28 pages (at 7 pages/min), scraped 27 items (at 7 items/min)
2017-12-14 16:49:38,185 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6240979.shtml
2017-12-14 16:49:38,187 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 16:49:38,188 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 9768,
 'downloader/request_count': 32,
 'downloader/request_method_count/GET': 32,
 'downloader/response_bytes': 1265083,
 'downloader/response_count': 29,
 'downloader/response_status_count/200': 29,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 8, 49, 38, 188116),
 'item_scraped_count': 28,
 'log_count/DEBUG': 125,
 'log_count/INFO': 36,
 'request_depth_max': 1,
 'response_received_count': 29,
 'scheduler/dequeued': 32,
 'scheduler/dequeued/memory': 32,
 'scheduler/enqueued': 32,
 'scheduler/enqueued/memory': 32,
 'start_time': datetime.datetime(2017, 12, 14, 8, 45, 38, 489190)}
2017-12-14 16:49:38,188 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 16:49:38,568 - loggers - INFO -  Starting crawl QqSpider
2017-12-14 16:49:40,022 - loggers - INFO - this is a test
2017-12-14 16:49:40,023 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 16:49:40,023 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 16:49:40,068 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 16:49:40,303 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 16:49:40,308 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 16:49:40,432 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 16:49:40,433 - scrapy.core.engine - INFO - Spider opened
2017-12-14 16:49:40,524 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 16:49:49,496 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/002191.htm
2017-12-14 16:50:00,343 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/005549.htm
2017-12-14 16:50:04,617 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/016269.htm
2017-12-14 16:50:06,087 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/016319.htm
2017-12-14 16:50:11,650 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/021154.htm
2017-12-14 16:50:14,503 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/011817.htm
2017-12-14 16:50:25,769 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/017053.htm
2017-12-14 16:50:26,805 - root - INFO - 已经爬过http://sports.qq.com/a/20170302/010011.htm
2017-12-14 16:50:29,883 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/000868.htm
2017-12-14 16:50:34,258 - root - INFO - 已经爬过http://sports.qq.com/a/20171003/007728.htm
2017-12-14 16:50:35,933 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/009426.htm
2017-12-14 16:50:40,529 - scrapy.extensions.logstats - INFO - Crawled 12 pages (at 12 pages/min), scraped 11 items (at 11 items/min)
2017-12-14 16:50:44,752 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/016941.htm
2017-12-14 16:50:45,274 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/003027.htm
2017-12-14 16:50:48,213 - root - INFO - 已经爬过http://sports.qq.com/a/20161019/029134.htm
2017-12-14 16:50:48,752 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/002486.htm
2017-12-14 16:50:53,845 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/001106.htm
2017-12-14 16:51:00,785 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/003278.htm
2017-12-14 16:51:05,381 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/012005.htm
2017-12-14 16:51:05,384 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 16:51:05,385 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 1,
 'downloader/request_bytes': 5307,
 'downloader/request_count': 20,
 'downloader/request_method_count/GET': 20,
 'downloader/response_bytes': 822618,
 'downloader/response_count': 19,
 'downloader/response_status_count/200': 19,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 8, 51, 5, 385122),
 'item_scraped_count': 18,
 'log_count/DEBUG': 79,
 'log_count/INFO': 26,
 'request_depth_max': 1,
 'response_received_count': 19,
 'scheduler/dequeued': 20,
 'scheduler/dequeued/memory': 20,
 'scheduler/enqueued': 20,
 'scheduler/enqueued/memory': 20,
 'start_time': datetime.datetime(2017, 12, 14, 8, 49, 40, 524304)}
2017-12-14 16:51:05,385 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 16:51:05,977 - loggers - INFO -  Starting crawl ChinanbaSpider
2017-12-14 16:51:06,880 - loggers - INFO - this is a test
2017-12-14 16:51:06,881 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 16:51:06,881 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 16:51:06,905 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 16:51:07,044 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 16:51:07,046 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 16:51:07,147 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 16:51:07,147 - scrapy.core.engine - INFO - Spider opened
2017-12-14 16:51:07,474 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 16:52:03,372 - scrapy.core.scraper - ERROR - Spider error processing <GET http://nbachina.qq.com/a/20171214/022673.htm> (referer: http://china.nba.com/news/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 43, in parse_detail
    publish_time = response.xpath(xpath.get_publishtime()).extract_first().strip()
AttributeError: 'NoneType' object has no attribute 'strip'
2017-12-14 16:52:07,476 - scrapy.extensions.logstats - INFO - Crawled 3 pages (at 3 pages/min), scraped 1 items (at 1 items/min)
2017-12-14 16:52:15,093 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/013572.htm
2017-12-14 16:52:19,268 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/013681.htm
2017-12-14 16:52:23,409 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/014719.htm
2017-12-14 16:52:27,364 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/014438.htm
2017-12-14 16:52:31,536 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/014875.htm
2017-12-14 16:52:34,746 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/015916.htm
2017-12-14 16:52:43,543 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/016840.htm
2017-12-14 16:52:47,144 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/016927.htm
2017-12-14 16:52:48,412 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/016020.htm
2017-12-14 16:52:50,739 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/016954.htm
2017-12-14 16:52:54,618 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/017103.htm
2017-12-14 16:53:02,427 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/017248.htm
2017-12-14 16:53:05,809 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/017281.htm
2017-12-14 16:53:07,476 - scrapy.extensions.logstats - INFO - Crawled 19 pages (at 16 pages/min), scraped 17 items (at 16 items/min)
2017-12-14 16:53:08,175 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/018644.htm
2017-12-14 16:54:07,476 - scrapy.extensions.logstats - INFO - Crawled 24 pages (at 5 pages/min), scraped 22 items (at 5 items/min)
2017-12-14 16:54:15,747 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/018186.htm
2017-12-14 16:54:15,751 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 16:54:15,752 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/request_bytes': 6720,
 'downloader/request_count': 25,
 'downloader/request_method_count/GET': 25,
 'downloader/response_bytes': 665860,
 'downloader/response_count': 25,
 'downloader/response_status_count/200': 25,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 8, 54, 15, 751869),
 'item_scraped_count': 23,
 'log_count/DEBUG': 99,
 'log_count/ERROR': 1,
 'log_count/INFO': 25,
 'request_depth_max': 1,
 'response_received_count': 25,
 'scheduler/dequeued': 25,
 'scheduler/dequeued/memory': 25,
 'scheduler/enqueued': 25,
 'scheduler/enqueued/memory': 25,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2017, 12, 14, 8, 51, 7, 474764)}
2017-12-14 16:54:15,752 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 16:54:16,386 - loggers - INFO -  Starting crawl ZhibobaSpider
2017-12-14 16:54:17,352 - loggers - INFO - this is a test
2017-12-14 16:54:17,352 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 16:54:17,353 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 16:54:17,375 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 16:54:17,509 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 16:54:17,511 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 16:54:17,610 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 16:54:17,610 - scrapy.core.engine - INFO - Spider opened
2017-12-14 16:54:17,691 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 16:54:27,119 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
scrapy.core.downloader.handlers.http11.TunnelError: Could not open CONNECT tunnel with proxy 120.27.49.85:8090 [{'status': 400, 'reason': b'Bad Request'}]
2017-12-14 16:54:27,227 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 16:54:27,228 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/scrapy.core.downloader.handlers.http11.TunnelError': 3,
 'downloader/request_bytes': 648,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 8, 54, 27, 227755),
 'log_count/DEBUG': 10,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 12, 14, 8, 54, 17, 691361)}
2017-12-14 16:54:27,228 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 18:13:39,593 - loggers - INFO -  Starting crawl SinaSpider
2017-12-14 18:13:40,959 - loggers - INFO - this is a test
2017-12-14 18:13:40,959 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 18:13:40,960 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 18:13:41,011 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 18:13:41,195 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 18:13:41,199 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 18:13:41,305 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 18:13:41,314 - scrapy.core.engine - INFO - Spider opened
2017-12-14 18:13:41,388 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 18:13:47,814 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7197127.shtml
2017-12-14 18:13:51,580 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6249797.shtml
2017-12-14 18:14:04,398 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4406922.shtml
2017-12-14 18:14:10,807 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4436015.shtml
2017-12-14 18:14:13,887 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4438577.shtml
2017-12-14 18:14:17,491 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6254501.shtml
2017-12-14 18:14:20,611 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6240979.shtml
2017-12-14 18:14:22,826 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4407128.shtml
2017-12-14 18:14:25,732 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7160180.shtml
2017-12-14 18:14:26,763 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7201812.shtml
2017-12-14 18:14:33,124 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4460742.shtml
2017-12-14 18:14:39,123 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4479877.shtml
2017-12-14 18:14:41,085 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7210740.shtml
2017-12-14 18:14:41,392 - scrapy.extensions.logstats - INFO - Crawled 14 pages (at 14 pages/min), scraped 13 items (at 13 items/min)
2017-12-14 18:14:49,166 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4461518.shtml
2017-12-14 18:15:02,598 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4461895.shtml
2017-12-14 18:15:10,533 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4431588.shtml
2017-12-14 18:15:13,500 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7315698.shtml
2017-12-14 18:15:14,689 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7364695.shtml
2017-12-14 18:15:18,849 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7388627.shtml
2017-12-14 18:15:38,854 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4460600.shtml
2017-12-14 18:15:41,389 - scrapy.extensions.logstats - INFO - Crawled 23 pages (at 9 pages/min), scraped 22 items (at 9 items/min)
2017-12-14 18:15:44,552 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6247792.shtml
2017-12-14 18:15:48,137 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7761121.shtml
2017-12-14 18:15:49,235 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7278660.shtml
2017-12-14 18:16:24,364 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4486680.shtml
2017-12-14 18:16:41,390 - scrapy.extensions.logstats - INFO - Crawled 28 pages (at 5 pages/min), scraped 27 items (at 5 items/min)
2017-12-14 18:17:41,392 - scrapy.extensions.logstats - INFO - Crawled 28 pages (at 0 pages/min), scraped 27 items (at 0 items/min)
2017-12-14 18:17:44,079 - scrapy.crawler - INFO - Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-12-14 18:17:44,080 - scrapy.core.engine - INFO - Closing spider (shutdown)
2017-12-14 18:17:44,355 - scrapy.crawler - INFO - Received SIG_UNBLOCK twice, forcing unclean shutdown
2017-12-14 18:17:44,359 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 7,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 4,
 'downloader/request_bytes': 10692,
 'downloader/request_count': 35,
 'downloader/request_method_count/GET': 35,
 'downloader/response_bytes': 1152839,
 'downloader/response_count': 28,
 'downloader/response_status_count/200': 28,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2017, 12, 14, 10, 17, 44, 358817),
 'item_scraped_count': 27,
 'log_count/DEBUG': 133,
 'log_count/INFO': 37,
 'request_depth_max': 1,
 'response_received_count': 28,
 'scheduler/dequeued': 35,
 'scheduler/dequeued/memory': 35,
 'scheduler/enqueued': 37,
 'scheduler/enqueued/memory': 37,
 'start_time': datetime.datetime(2017, 12, 14, 10, 13, 41, 388007)}
2017-12-14 18:17:44,359 - scrapy.core.engine - INFO - Spider closed (shutdown)
2017-12-14 18:17:44,919 - loggers - INFO -  Starting crawl QqSpider
2017-12-14 18:17:45,431 - loggers - INFO -  Starting crawl ChinanbaSpider
2017-12-14 18:17:45,827 - loggers - INFO -  Starting crawl ZhibobaSpider
2017-12-14 21:50:35,727 - loggers - INFO -  Starting crawl SinaSpider
2017-12-14 21:50:37,185 - loggers - INFO - this is a test
2017-12-14 21:50:37,186 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 21:50:37,186 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 21:50:37,236 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 21:50:37,437 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 21:50:37,441 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 21:50:37,563 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 21:50:37,564 - scrapy.core.engine - INFO - Spider opened
2017-12-14 21:50:38,488 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 21:50:42,197 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 21:50:42,198 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/request_bytes': 432,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 8880,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/301': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 13, 50, 42, 198252),
 'log_count/DEBUG': 7,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 12, 14, 13, 50, 38, 488782)}
2017-12-14 21:50:42,198 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 21:50:42,707 - loggers - INFO -  Starting crawl QqSpider
2017-12-14 21:50:43,665 - loggers - INFO - this is a test
2017-12-14 21:50:43,666 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 21:50:43,666 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 21:50:43,695 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 21:50:43,848 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 21:50:43,850 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 21:50:43,959 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 21:50:43,959 - scrapy.core.engine - INFO - Spider opened
2017-12-14 21:50:44,120 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 21:50:48,780 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/003278.htm
2017-12-14 21:50:52,482 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/005549.htm
2017-12-14 21:50:55,539 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/016269.htm
2017-12-14 21:50:58,449 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/016319.htm
2017-12-14 21:51:06,094 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/021154.htm
2017-12-14 21:51:06,357 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/000868.htm
2017-12-14 21:51:09,504 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/011817.htm
2017-12-14 21:51:12,670 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/017053.htm
2017-12-14 21:51:17,040 - root - INFO - 已经爬过http://sports.qq.com/a/20170302/010011.htm
2017-12-14 21:51:21,203 - root - INFO - 已经爬过http://sports.qq.com/a/20161019/029134.htm
2017-12-14 21:51:30,220 - root - INFO - 已经爬过http://sports.qq.com/a/20171003/007728.htm
2017-12-14 21:51:33,686 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/009426.htm
2017-12-14 21:51:33,877 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/012005.htm
2017-12-14 21:51:42,367 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/003027.htm
2017-12-14 21:51:44,122 - scrapy.extensions.logstats - INFO - Crawled 15 pages (at 15 pages/min), scraped 14 items (at 14 items/min)
2017-12-14 21:51:44,326 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/016941.htm
2017-12-14 21:51:48,620 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/002486.htm
2017-12-14 21:51:53,176 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/002191.htm
2017-12-14 21:52:44,124 - scrapy.extensions.logstats - INFO - Crawled 18 pages (at 3 pages/min), scraped 17 items (at 3 items/min)
2017-12-14 21:53:44,122 - scrapy.extensions.logstats - INFO - Crawled 18 pages (at 0 pages/min), scraped 17 items (at 0 items/min)
2017-12-14 21:54:44,120 - scrapy.extensions.logstats - INFO - Crawled 18 pages (at 0 pages/min), scraped 17 items (at 0 items/min)
2017-12-14 21:55:06,442 - root - INFO - 已经爬过http://sports.qq.com/a/20171214/001106.htm
2017-12-14 21:55:06,450 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 21:55:06,452 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/request_bytes': 5307,
 'downloader/request_count': 20,
 'downloader/request_method_count/GET': 20,
 'downloader/response_bytes': 716200,
 'downloader/response_count': 20,
 'downloader/response_status_count/200': 19,
 'downloader/response_status_count/504': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 13, 55, 6, 451774),
 'item_scraped_count': 18,
 'log_count/DEBUG': 79,
 'log_count/INFO': 29,
 'request_depth_max': 1,
 'response_received_count': 19,
 'scheduler/dequeued': 20,
 'scheduler/dequeued/memory': 20,
 'scheduler/enqueued': 20,
 'scheduler/enqueued/memory': 20,
 'start_time': datetime.datetime(2017, 12, 14, 13, 50, 44, 120244)}
2017-12-14 21:55:06,452 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 21:55:06,998 - loggers - INFO -  Starting crawl ChinanbaSpider
2017-12-14 21:55:08,338 - loggers - INFO - this is a test
2017-12-14 21:55:08,339 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 21:55:08,339 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 21:55:08,381 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 21:55:08,635 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 21:55:08,640 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 21:55:08,763 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 21:55:08,763 - scrapy.core.engine - INFO - Spider opened
2017-12-14 21:55:09,122 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 21:55:23,859 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/024011.htm
2017-12-14 21:55:25,872 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/013572.htm
2017-12-14 21:55:32,972 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/014719.htm
2017-12-14 21:55:36,323 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/013681.htm
2017-12-14 21:55:37,007 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/014875.htm
2017-12-14 21:55:42,148 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/015916.htm
2017-12-14 21:55:44,891 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/016020.htm
2017-12-14 21:55:48,392 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/016840.htm
2017-12-14 21:55:53,940 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/016927.htm
2017-12-14 21:55:58,605 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/017103.htm
2017-12-14 21:56:00,416 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/016954.htm
2017-12-14 21:56:02,147 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/017248.htm
2017-12-14 21:56:06,684 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/017281.htm
2017-12-14 21:56:08,194 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/018186.htm
2017-12-14 21:56:09,126 - scrapy.extensions.logstats - INFO - Crawled 15 pages (at 15 pages/min), scraped 14 items (at 14 items/min)
2017-12-14 21:56:11,968 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/018644.htm
2017-12-14 21:56:17,291 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/020594.htm
2017-12-14 21:56:23,823 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/021241.htm
2017-12-14 21:56:30,891 - scrapy.core.scraper - ERROR - Spider error processing <GET http://www.youdao.com/> (referer: http://china.nba.com/news/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 40, in parse_detail
    item[TITLE] = response.xpath(xpath.get_title()).extract_first().strip()
AttributeError: 'NoneType' object has no attribute 'strip'
2017-12-14 21:56:32,998 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/022321.htm
2017-12-14 21:56:36,690 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/022401.htm
2017-12-14 21:56:42,266 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/022435.htm
2017-12-14 21:56:48,241 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/022971.htm
2017-12-14 21:57:01,867 - root - INFO - 已经爬过http://nbachina.qq.com/a/20171214/022316.htm
2017-12-14 21:57:01,869 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 21:57:01,870 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/request_bytes': 6969,
 'downloader/request_count': 26,
 'downloader/request_method_count/GET': 26,
 'downloader/response_bytes': 681377,
 'downloader/response_count': 26,
 'downloader/response_status_count/200': 25,
 'downloader/response_status_count/301': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 13, 57, 1, 870188),
 'item_scraped_count': 23,
 'log_count/DEBUG': 102,
 'log_count/ERROR': 1,
 'log_count/INFO': 30,
 'request_depth_max': 1,
 'response_received_count': 25,
 'scheduler/dequeued': 26,
 'scheduler/dequeued/memory': 26,
 'scheduler/enqueued': 26,
 'scheduler/enqueued/memory': 26,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2017, 12, 14, 13, 55, 9, 121998)}
2017-12-14 21:57:01,870 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 21:57:02,497 - loggers - INFO -  Starting crawl ZhibobaSpider
2017-12-14 21:57:03,656 - loggers - INFO - this is a test
2017-12-14 21:57:03,656 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 21:57:03,657 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 21:57:03,692 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 21:57:03,901 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 21:57:03,906 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 21:57:04,024 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 21:57:04,025 - scrapy.core.engine - INFO - Spider opened
2017-12-14 21:57:04,816 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 21:58:04,821 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 21:59:04,821 - scrapy.extensions.logstats - INFO - Crawled 4 pages (at 4 pages/min), scraped 3 items (at 3 items/min)
2017-12-14 21:59:36,915 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a31f65ca125d.htm
2017-12-14 21:59:49,851 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a3208cbaf129.htm
2017-12-14 22:00:04,820 - scrapy.extensions.logstats - INFO - Crawled 6 pages (at 2 pages/min), scraped 5 items (at 2 items/min)
2017-12-14 22:00:08,414 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a320c02cf3bf.htm
2017-12-14 22:00:11,604 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a31eff626dbb.htm
2017-12-14 22:00:37,577 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/106659.htm
2017-12-14 22:00:48,144 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a31fedd29a6c.htm
2017-12-14 22:01:00,458 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/106666.htm
2017-12-14 22:01:03,506 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a3206033797d.htm
2017-12-14 22:01:04,818 - scrapy.extensions.logstats - INFO - Crawled 15 pages (at 9 pages/min), scraped 14 items (at 9 items/min)
2017-12-14 22:01:10,352 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a320563177eb.htm
2017-12-14 22:01:20,004 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a31fb3e0eb18.htm
2017-12-14 22:01:29,573 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/106664.htm
2017-12-14 22:01:36,334 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a31e58a46067.htm
2017-12-14 22:01:57,819 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-13/5a312a9360ee4.htm
2017-12-14 22:02:04,821 - scrapy.extensions.logstats - INFO - Crawled 24 pages (at 9 pages/min), scraped 23 items (at 9 items/min)
2017-12-14 22:02:06,829 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2017-12-14/5a31f124b2aed.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
scrapy.core.downloader.handlers.http11.TunnelError: Could not open CONNECT tunnel with proxy 163.125.195.67:8118 [{'status': 503, 'reason': b'Too many open connections'}]
2017-12-14 22:02:15,789 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2017-12-14/5a324e3a11d2a.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
scrapy.core.downloader.handlers.http11.TunnelError: Could not open CONNECT tunnel with proxy 61.135.217.7:80 [{'status': 400, 'reason': b'Bad Request'}]
2017-12-14 22:02:20,949 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2017-12-14/5a31eb38c5c4e.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
scrapy.core.downloader.handlers.http11.TunnelError: Could not open CONNECT tunnel with proxy 118.193.107.2:80 [{'status': 400, 'reason': b'Bad Request'}]
2017-12-14 22:02:22,581 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/5a3229a77ffa1.htm
2017-12-14 22:02:26,422 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/106667.htm
2017-12-14 22:02:30,351 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2017-12-14/5a323623123d4.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
scrapy.core.downloader.handlers.http11.TunnelError: Could not open CONNECT tunnel with proxy 120.78.52.212:8000 [{'status': 403, 'reason': b'Forbidden'}]
2017-12-14 22:02:41,201 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2017-12-14/109267.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
scrapy.core.downloader.handlers.http11.TunnelError: Could not open CONNECT tunnel with proxy 118.193.107.234:80 [{'status': 400, 'reason': b'Bad Request'}]
2017-12-14 22:03:04,821 - scrapy.extensions.logstats - INFO - Crawled 26 pages (at 2 pages/min), scraped 25 items (at 2 items/min)
2017-12-14 22:03:06,440 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2017-12-14/106665.htm
2017-12-14 22:04:04,820 - scrapy.extensions.logstats - INFO - Crawled 27 pages (at 1 pages/min), scraped 26 items (at 1 items/min)
2017-12-14 22:05:00,497 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2017-12-14/5a31f254e97bf.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://news.zhibo8.cc/nba/2017-12-14/5a31f254e97bf.htm took longer than 180.0 seconds..
2017-12-14 22:05:04,820 - scrapy.extensions.logstats - INFO - Crawled 27 pages (at 0 pages/min), scraped 26 items (at 0 items/min)
2017-12-14 22:05:10,993 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2017-12-14/5a32499fa43f3.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://news.zhibo8.cc/nba/2017-12-14/5a32499fa43f3.htm took longer than 180.0 seconds..
2017-12-14 22:05:11,099 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 22:05:11,101 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 42,
 'downloader/exception_type_count/scrapy.core.downloader.handlers.http11.TunnelError': 34,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 1,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 3,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 19127,
 'downloader/request_count': 69,
 'downloader/request_method_count/GET': 69,
 'downloader/response_bytes': 209314,
 'downloader/response_count': 27,
 'downloader/response_status_count/200': 27,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 14, 5, 11, 100916),
 'item_scraped_count': 26,
 'log_count/DEBUG': 234,
 'log_count/ERROR': 7,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 27,
 'scheduler/dequeued': 69,
 'scheduler/dequeued/memory': 69,
 'scheduler/enqueued': 69,
 'scheduler/enqueued/memory': 69,
 'start_time': datetime.datetime(2017, 12, 14, 13, 57, 4, 816812)}
2017-12-14 22:05:11,102 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 22:20:43,711 - loggers - INFO -  Starting crawl SinaSpider
2017-12-14 22:20:45,083 - loggers - INFO - this is a test
2017-12-14 22:20:45,083 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 22:20:45,084 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 22:20:45,128 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 22:20:45,350 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 22:20:45,354 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 22:20:45,477 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 22:20:45,477 - scrapy.core.engine - INFO - Spider opened
2017-12-14 22:20:45,572 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 22:21:23,668 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6249797.shtml
2017-12-14 22:21:24,940 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4436015.shtml
2017-12-14 22:21:33,992 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6240979.shtml
2017-12-14 22:21:45,576 - scrapy.extensions.logstats - INFO - Crawled 4 pages (at 4 pages/min), scraped 3 items (at 3 items/min)
2017-12-14 22:21:46,730 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7210740.shtml
2017-12-14 22:21:49,589 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4460600.shtml
2017-12-14 22:21:58,043 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4461518.shtml
2017-12-14 22:22:00,926 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4479877.shtml
2017-12-14 22:22:14,424 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4515805.shtml
2017-12-14 22:22:20,233 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz8186539.shtml
2017-12-14 22:22:24,770 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6251370.shtml
2017-12-14 22:22:26,497 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4431588.shtml
2017-12-14 22:22:29,483 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7278660.shtml
2017-12-14 22:22:33,639 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7315698.shtml
2017-12-14 22:22:35,931 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7364695.shtml
2017-12-14 22:22:44,942 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7388627.shtml
2017-12-14 22:22:45,014 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7761121.shtml
2017-12-14 22:22:45,575 - scrapy.extensions.logstats - INFO - Crawled 19 pages (at 15 pages/min), scraped 18 items (at 15 items/min)
2017-12-14 22:23:00,546 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6247792.shtml
2017-12-14 22:23:03,734 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7201812.shtml
2017-12-14 22:23:09,553 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6254501.shtml
2017-12-14 22:23:10,410 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4407128.shtml
2017-12-14 22:23:12,010 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4406922.shtml
2017-12-14 22:23:16,277 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4438577.shtml
2017-12-14 22:23:20,798 - scrapy.crawler - INFO - Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-12-14 22:23:20,798 - scrapy.core.engine - INFO - Closing spider (shutdown)
2017-12-14 22:23:21,025 - scrapy.crawler - INFO - Received SIG_UNBLOCK twice, forcing unclean shutdown
2017-12-14 22:23:21,189 - loggers - INFO -  Starting crawl QqSpider
2017-12-14 22:23:21,964 - loggers - INFO -  Starting crawl ChinanbaSpider
2017-12-14 22:23:22,177 - loggers - INFO -  Starting crawl ZhibobaSpider
2017-12-14 22:23:24,774 - loggers - INFO -  Starting crawl SinaSpider
2017-12-14 22:23:25,843 - loggers - INFO - this is a test
2017-12-14 22:23:25,844 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 22:23:25,845 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 22:23:25,891 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 22:23:26,129 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 22:23:26,134 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 22:23:26,258 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 22:23:26,258 - scrapy.core.engine - INFO - Spider opened
2017-12-14 22:23:26,404 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 22:24:00,273 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4436015.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 40, in parse_detail
    item[TITLE] = response.xpath(xpath.get_title()).extract_first().strip()
AttributeError: 'NoneType' object has no attribute 'strip'
2017-12-14 22:24:26,502 - scrapy.extensions.logstats - INFO - Crawled 11 pages (at 11 pages/min), scraped 8 items (at 8 items/min)
2017-12-14 22:25:02,981 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7315698.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 40, in parse_detail
    item[TITLE] = response.xpath(xpath.get_title()).extract_first().strip()
AttributeError: 'NoneType' object has no attribute 'strip'
2017-12-14 22:25:26,405 - scrapy.extensions.logstats - INFO - Crawled 26 pages (at 15 pages/min), scraped 23 items (at 15 items/min)
2017-12-14 22:25:51,456 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7210740.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:25:54,748 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4514438.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:25:59,020 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7330474.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:26:01,320 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6247792.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:26:01,424 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 22:26:01,425 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 12,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 11,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/request_bytes': 11937,
 'downloader/request_count': 39,
 'downloader/request_method_count/GET': 39,
 'downloader/response_bytes': 791384,
 'downloader/response_count': 27,
 'downloader/response_status_count/200': 26,
 'downloader/response_status_count/302': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 14, 26, 1, 425102),
 'item_scraped_count': 23,
 'log_count/DEBUG': 141,
 'log_count/ERROR': 6,
 'log_count/INFO': 9,
 'request_depth_max': 1,
 'response_received_count': 26,
 'scheduler/dequeued': 39,
 'scheduler/dequeued/memory': 39,
 'scheduler/enqueued': 39,
 'scheduler/enqueued/memory': 39,
 'spider_exceptions/AttributeError': 2,
 'start_time': datetime.datetime(2017, 12, 14, 14, 23, 26, 404684)}
2017-12-14 22:26:01,426 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 22:26:01,867 - loggers - INFO -  Starting crawl QqSpider
2017-12-14 22:26:03,009 - loggers - INFO - this is a test
2017-12-14 22:26:03,009 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 22:26:03,010 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 22:26:03,054 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 22:26:03,294 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 22:26:03,300 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 22:26:03,436 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 22:26:03,436 - scrapy.core.engine - INFO - Spider opened
2017-12-14 22:26:03,573 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 22:26:13,143 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.qq.com/nba/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:26:13,250 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 22:26:13,252 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 645,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 14, 26, 13, 251528),
 'log_count/DEBUG': 10,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 12, 14, 14, 26, 3, 573833)}
2017-12-14 22:26:13,252 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 22:26:13,668 - loggers - INFO -  Starting crawl ChinanbaSpider
2017-12-14 22:26:14,743 - loggers - INFO - this is a test
2017-12-14 22:26:14,744 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 22:26:14,744 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 22:26:14,770 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 22:26:14,900 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 22:26:14,902 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 22:26:15,002 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 22:26:15,003 - scrapy.core.engine - INFO - Spider opened
2017-12-14 22:26:15,119 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 22:26:24,352 - scrapy.core.scraper - ERROR - Error downloading <GET http://china.nba.com/news/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:26:24,457 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 22:26:24,458 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 648,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 14, 26, 24, 458496),
 'log_count/DEBUG': 10,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 12, 14, 14, 26, 15, 119159)}
2017-12-14 22:26:24,459 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 22:26:24,716 - loggers - INFO -  Starting crawl ZhibobaSpider
2017-12-14 22:26:25,632 - loggers - INFO - this is a test
2017-12-14 22:26:25,633 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 22:26:25,633 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 22:26:25,657 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 22:26:25,788 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 22:26:25,790 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 22:26:25,881 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 22:26:25,882 - scrapy.core.engine - INFO - Spider opened
2017-12-14 22:26:25,998 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 22:26:33,644 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:26:33,757 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 22:26:33,758 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 648,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 14, 26, 33, 758030),
 'log_count/DEBUG': 10,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 12, 14, 14, 26, 25, 998725)}
2017-12-14 22:26:33,758 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 22:33:57,250 - loggers - INFO -  Starting crawl SinaSpider
2017-12-14 22:33:58,414 - loggers - INFO - this is a test
2017-12-14 22:33:58,415 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 22:33:58,415 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 22:33:58,452 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 22:33:58,651 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 22:33:58,656 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 22:33:58,783 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 22:33:58,784 - scrapy.core.engine - INFO - Spider opened
2017-12-14 22:34:01,959 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 22:34:25,046 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7197127.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7197127.shtml took longer than 15.0 seconds..
2017-12-14 22:34:26,757 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6249797.shtml
2017-12-14 22:34:29,844 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4406922.shtml
2017-12-14 22:34:32,286 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4407128.shtml
2017-12-14 22:34:37,624 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4438577.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 40, in parse_detail
    item[TITLE] = response.xpath(xpath.get_title()).extract_first().strip()
AttributeError: 'NoneType' object has no attribute 'strip'
2017-12-14 22:34:48,980 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4436015.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4436015.shtml took longer than 15.0 seconds..
2017-12-14 22:34:55,175 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6240979.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6240979.shtml took longer than 15.0 seconds..
2017-12-14 22:35:01,513 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4460600.shtml
2017-12-14 22:35:03,558 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6254501.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6254501.shtml took longer than 15.0 seconds..
2017-12-14 22:35:03,559 - scrapy.extensions.logstats - INFO - Crawled 8 pages (at 8 pages/min), scraped 4 items (at 4 items/min)
2017-12-14 22:35:04,680 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7201812.shtml
2017-12-14 22:35:05,907 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4460742.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 40, in parse_detail
    item[TITLE] = response.xpath(xpath.get_title()).extract_first().strip()
AttributeError: 'NoneType' object has no attribute 'strip'
2017-12-14 22:35:16,255 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4461518.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2017-12-14 22:35:17,848 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz8186539.shtml
2017-12-14 22:35:18,485 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4479877.shtml
2017-12-14 22:35:19,088 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7345085.shtml
2017-12-14 22:35:28,007 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz6251370.shtml
2017-12-14 22:35:35,599 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4431588.shtml
2017-12-14 22:35:35,705 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4515805.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2017-12-14 22:35:39,622 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz8635927.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2017-12-14 22:35:50,708 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7278660.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7278660.shtml took longer than 15.0 seconds..
2017-12-14 22:35:52,918 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7761121.shtml
2017-12-14 22:35:53,748 - root - INFO - 已经爬过http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifyptkyk4486680.shtml
2017-12-14 22:36:01,532 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7388627.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2017-12-14/doc-ifypsqiz7388627.shtml took longer than 15.0 seconds..
2017-12-14 22:36:01,959 - scrapy.extensions.logstats - INFO - Crawled 18 pages (at 10 pages/min), scraped 15 items (at 11 items/min)
2017-12-14 22:36:13,442 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 22:36:13,443 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 9,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 8,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 9152,
 'downloader/request_count': 30,
 'downloader/request_method_count/GET': 30,
 'downloader/response_bytes': 565136,
 'downloader/response_count': 21,
 'downloader/response_status_count/200': 19,
 'downloader/response_status_count/500': 1,
 'downloader/response_status_count/504': 1,
 'dupefilter/filtered': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 14, 36, 13, 442845),
 'item_scraped_count': 16,
 'log_count/DEBUG': 101,
 'log_count/ERROR': 11,
 'log_count/INFO': 21,
 'request_depth_max': 1,
 'response_received_count': 19,
 'scheduler/dequeued': 30,
 'scheduler/dequeued/memory': 30,
 'scheduler/enqueued': 30,
 'scheduler/enqueued/memory': 30,
 'spider_exceptions/AttributeError': 2,
 'start_time': datetime.datetime(2017, 12, 14, 14, 34, 1, 959484)}
2017-12-14 22:36:13,443 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 22:36:14,003 - loggers - INFO -  Starting crawl QqSpider
2017-12-14 22:36:15,415 - loggers - INFO - this is a test
2017-12-14 22:36:15,415 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 22:36:15,416 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 22:36:15,456 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 22:36:15,673 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 22:36:15,681 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 22:36:15,810 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 22:36:15,811 - scrapy.core.engine - INFO - Spider opened
2017-12-14 22:36:16,000 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 22:36:43,346 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.qq.com/a/20171214/016941.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.qq.com/a/20171214/016941.htm took longer than 15.0 seconds..
2017-12-14 22:37:16,004 - scrapy.extensions.logstats - INFO - Crawled 7 pages (at 7 pages/min), scraped 6 items (at 6 items/min)
2017-12-14 22:37:40,354 - scrapy.core.scraper - ERROR - Spider error processing <GET http://www.youdao.com/> (referer: http://sports.qq.com/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 40, in parse_detail
    item[TITLE] = response.xpath(xpath.get_title()).extract_first().strip()
AttributeError: 'NoneType' object has no attribute 'strip'
2017-12-14 22:37:49,568 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 22:37:49,569 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/request_bytes': 5287,
 'downloader/request_count': 20,
 'downloader/request_method_count/GET': 20,
 'downloader/response_bytes': 534294,
 'downloader/response_count': 19,
 'downloader/response_status_count/200': 15,
 'downloader/response_status_count/301': 2,
 'downloader/response_status_count/403': 2,
 'dupefilter/filtered': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 14, 37, 49, 568978),
 'item_scraped_count': 13,
 'log_count/DEBUG': 76,
 'log_count/ERROR': 2,
 'log_count/INFO': 8,
 'request_depth_max': 1,
 'response_received_count': 15,
 'scheduler/dequeued': 20,
 'scheduler/dequeued/memory': 20,
 'scheduler/enqueued': 20,
 'scheduler/enqueued/memory': 20,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2017, 12, 14, 14, 36, 16, 553)}
2017-12-14 22:37:49,570 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 22:37:50,066 - loggers - INFO -  Starting crawl ChinanbaSpider
2017-12-14 22:37:50,998 - loggers - INFO - this is a test
2017-12-14 22:37:50,999 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 22:37:50,999 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 22:37:51,023 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 22:37:51,178 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 22:37:51,180 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 22:37:51,283 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 22:37:51,283 - scrapy.core.engine - INFO - Spider opened
2017-12-14 22:37:51,380 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 22:38:09,832 - scrapy.core.scraper - ERROR - Error downloading <GET http://nbachina.qq.com/a/20171214/024011.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://nbachina.qq.com/a/20171214/024011.htm took longer than 15.0 seconds..
2017-12-14 22:38:47,509 - scrapy.core.scraper - ERROR - Error downloading <GET http://nbachina.qq.com/a/20171214/017248.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:38:50,747 - scrapy.core.scraper - ERROR - Error downloading <GET http://nbachina.qq.com/a/20171214/017281.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:38:51,385 - scrapy.extensions.logstats - INFO - Crawled 12 pages (at 12 pages/min), scraped 11 items (at 11 items/min)
2017-12-14 22:38:54,567 - scrapy.core.scraper - ERROR - Error downloading <GET http://nbachina.qq.com/a/20171214/018186.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:38:57,470 - scrapy.core.scraper - ERROR - Error downloading <GET http://nbachina.qq.com/a/20171214/018644.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:39:01,589 - scrapy.core.scraper - ERROR - Error downloading <GET http://nbachina.qq.com/a/20171214/020594.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:39:05,329 - scrapy.core.scraper - ERROR - Error downloading <GET http://nbachina.qq.com/a/20171214/021241.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:39:09,655 - scrapy.core.scraper - ERROR - Error downloading <GET http://nbachina.qq.com/a/20171214/022316.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:39:13,703 - scrapy.core.scraper - ERROR - Error downloading <GET http://nbachina.qq.com/a/20171214/022321.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:39:15,696 - scrapy.core.scraper - ERROR - Error downloading <GET http://nbachina.qq.com/a/20171214/022401.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:39:19,384 - scrapy.core.scraper - ERROR - Error downloading <GET http://nbachina.qq.com/a/20171214/022435.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:39:23,591 - scrapy.core.scraper - ERROR - Error downloading <GET http://nbachina.qq.com/a/20171214/022673.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:39:27,980 - scrapy.core.scraper - ERROR - Error downloading <GET http://nbachina.qq.com/a/20171214/022971.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:39:28,084 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 22:39:28,085 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 13,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 12,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/request_bytes': 6720,
 'downloader/request_count': 25,
 'downloader/request_method_count/GET': 25,
 'downloader/response_bytes': 328152,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 12,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 14, 39, 28, 85095),
 'item_scraped_count': 11,
 'log_count/DEBUG': 74,
 'log_count/ERROR': 13,
 'log_count/INFO': 8,
 'request_depth_max': 1,
 'response_received_count': 12,
 'scheduler/dequeued': 25,
 'scheduler/dequeued/memory': 25,
 'scheduler/enqueued': 25,
 'scheduler/enqueued/memory': 25,
 'start_time': datetime.datetime(2017, 12, 14, 14, 37, 51, 380120)}
2017-12-14 22:39:28,085 - scrapy.core.engine - INFO - Spider closed (finished)
2017-12-14 22:39:28,682 - loggers - INFO -  Starting crawl ZhibobaSpider
2017-12-14 22:39:29,487 - loggers - INFO - this is a test
2017-12-14 22:39:29,487 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2017-12-14 22:39:29,488 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2017-12-14 22:39:29,511 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-12-14 22:39:29,629 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-12-14 22:39:29,631 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-12-14 22:39:29,720 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2017-12-14 22:39:29,720 - scrapy.core.engine - INFO - Spider opened
2017-12-14 22:39:29,853 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-12-14 22:39:30,978 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2017-12-14 22:39:31,085 - scrapy.core.engine - INFO - Closing spider (finished)
2017-12-14 22:39:31,086 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,
 'downloader/request_bytes': 216,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 12, 14, 14, 39, 31, 85969),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 12, 14, 14, 39, 29, 852997)}
2017-12-14 22:39:31,086 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-08 21:03:01,673 - loggers - INFO -  Starting crawl SinaSpider
2018-05-08 21:03:03,340 - loggers - INFO - this is a test
2018-05-08 21:03:03,341 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-08 21:03:03,341 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-08 21:03:03,391 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-08 21:03:03,691 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-08 21:03:03,696 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-08 21:03:03,813 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-08 21:03:03,813 - scrapy.core.engine - INFO - Spider opened
2018-05-08 21:03:03,922 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-08 21:03:10,256 - loggers - INFO -  Starting crawl QqSpider
2018-05-08 21:03:11,676 - loggers - INFO - this is a test
2018-05-08 21:03:11,677 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-08 21:03:11,677 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-08 21:03:11,720 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-08 21:03:11,975 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-08 21:03:11,980 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-08 21:03:12,081 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-08 21:03:12,082 - scrapy.core.engine - INFO - Spider opened
2018-05-08 21:03:12,176 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-08 21:03:15,025 - scrapy.crawler - INFO - Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2018-05-08 22:16:17,498 - loggers - INFO -  Starting crawl SinaSpider
2018-05-08 22:16:18,914 - loggers - INFO - this is a test
2018-05-08 22:16:18,915 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-08 22:16:18,915 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-08 22:16:18,959 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-08 22:16:19,215 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-08 22:16:19,220 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-08 22:16:19,333 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-08 22:16:19,333 - scrapy.core.engine - INFO - Spider opened
2018-05-08 22:16:19,822 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-08 22:16:20,987 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/nba/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2018-05-08 22:16:21,096 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-08 22:16:21,098 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,
 'downloader/request_bytes': 220,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 8, 14, 16, 21, 97900),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 5, 8, 14, 16, 19, 822392)}
2018-05-08 22:16:21,098 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-08 22:16:21,641 - loggers - INFO -  Starting crawl QqSpider
2018-05-08 22:16:22,781 - loggers - INFO - this is a test
2018-05-08 22:16:22,782 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-08 22:16:22,783 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-08 22:16:22,810 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-08 22:16:23,000 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-08 22:16:23,005 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-08 22:16:23,132 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-08 22:16:23,134 - scrapy.core.engine - INFO - Spider opened
2018-05-08 22:16:23,236 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-08 22:16:24,364 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.qq.com/nba/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2018-05-08 22:16:24,470 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-08 22:16:24,471 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,
 'downloader/request_bytes': 215,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 8, 14, 16, 24, 471383),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 5, 8, 14, 16, 23, 236069)}
2018-05-08 22:16:24,471 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-08 22:16:24,835 - loggers - INFO -  Starting crawl ChinanbaSpider
2018-05-08 22:16:25,994 - loggers - INFO - this is a test
2018-05-08 22:16:25,994 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-08 22:16:25,995 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-08 22:16:26,025 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-08 22:16:26,182 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-08 22:16:26,186 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-08 22:16:26,261 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-08 22:16:26,262 - scrapy.core.engine - INFO - Spider opened
2018-05-08 22:16:26,689 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-08 22:16:27,816 - scrapy.core.scraper - ERROR - Error downloading <GET http://china.nba.com/news/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2018-05-08 22:16:27,922 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-08 22:16:27,923 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,
 'downloader/request_bytes': 216,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 8, 14, 16, 27, 922789),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 5, 8, 14, 16, 26, 689252)}
2018-05-08 22:16:27,923 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-08 22:16:28,512 - loggers - INFO -  Starting crawl ZhibobaSpider
2018-05-08 22:16:29,641 - loggers - INFO - this is a test
2018-05-08 22:16:29,641 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-08 22:16:29,642 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-08 22:16:29,668 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-08 22:16:29,669 - twisted - CRITICAL - Unhandled error in Deferred:
2018-05-08 22:16:29,669 - twisted - CRITICAL - 
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 96, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/ZhibobaSpider.py", line 16, in __init__
    super().__init__(config=self.config)
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 17, in __init__
    self.data = json2Dict(config_path)
  File "/Users/vincent/pyProjects/scrape/utils/json2Dict.py", line 45, in __init__
    self.start_url = json_data[START_URL]
TypeError: 'NoneType' object is not subscriptable
2018-05-08 22:17:24,745 - loggers - INFO -  Starting crawl test
2018-05-08 22:17:51,395 - loggers - INFO -  Starting crawl SinaSpider
2018-05-08 22:17:52,522 - loggers - INFO - this is a test
2018-05-08 22:17:52,523 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-08 22:17:52,524 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-08 22:17:52,548 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-08 22:17:52,716 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-08 22:17:52,720 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-08 22:17:52,798 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-08 22:17:52,798 - scrapy.core.engine - INFO - Spider opened
2018-05-08 22:17:52,892 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-08 22:17:54,022 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/nba/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2018-05-08 22:17:54,130 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-08 22:17:54,131 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,
 'downloader/request_bytes': 220,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 8, 14, 17, 54, 131134),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 5, 8, 14, 17, 52, 892474)}
2018-05-08 22:17:54,131 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-08 22:17:54,485 - loggers - INFO -  Starting crawl QqSpider
2018-05-08 22:17:55,673 - loggers - INFO - this is a test
2018-05-08 22:17:55,674 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-08 22:17:55,674 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-08 22:17:55,702 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-08 22:17:55,880 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-08 22:17:55,884 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-08 22:17:55,964 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-08 22:17:55,964 - scrapy.core.engine - INFO - Spider opened
2018-05-08 22:17:56,467 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-08 22:17:57,592 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.qq.com/nba/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2018-05-08 22:17:57,697 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-08 22:17:57,698 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,
 'downloader/request_bytes': 215,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 8, 14, 17, 57, 697724),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 5, 8, 14, 17, 56, 467357)}
2018-05-08 22:17:57,698 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-08 22:17:58,107 - loggers - INFO -  Starting crawl ChinanbaSpider
2018-05-08 22:17:58,496 - loggers - INFO -  Starting crawl ZhibobaSpider
2018-05-08 22:17:59,617 - loggers - INFO - this is a test
2018-05-08 22:17:59,617 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-08 22:17:59,618 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-08 22:18:04,915 - loggers - INFO -  Starting crawl SinaSpider
2018-05-08 22:18:06,007 - loggers - INFO - this is a test
2018-05-08 22:18:06,007 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-08 22:18:06,008 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-08 22:18:06,031 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-08 22:18:06,185 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-08 22:18:06,188 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-08 22:18:06,263 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-08 22:18:06,263 - scrapy.core.engine - INFO - Spider opened
2018-05-08 22:18:06,365 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-08 22:18:07,496 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/nba/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2018-05-08 22:18:07,603 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-08 22:18:07,604 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,
 'downloader/request_bytes': 220,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 8, 14, 18, 7, 603974),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 5, 8, 14, 18, 6, 365489)}
2018-05-08 22:18:07,604 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-08 22:18:07,931 - loggers - INFO -  Starting crawl QqSpider
2018-05-08 22:18:09,020 - loggers - INFO - this is a test
2018-05-08 22:18:09,021 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-08 22:18:09,021 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-08 22:18:09,047 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-08 22:18:09,224 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-08 22:18:09,228 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-08 22:18:09,319 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-08 22:18:09,319 - scrapy.core.engine - INFO - Spider opened
2018-05-08 22:18:09,411 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-08 22:18:10,537 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.qq.com/nba/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2018-05-08 22:18:10,644 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-08 22:18:10,645 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,
 'downloader/request_bytes': 215,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 8, 14, 18, 10, 645257),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 5, 8, 14, 18, 9, 411207)}
2018-05-08 22:18:10,646 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-08 22:18:11,002 - loggers - INFO -  Starting crawl ChinanbaSpider
2018-05-08 22:18:12,108 - loggers - INFO - this is a test
2018-05-08 22:18:12,109 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-08 22:18:12,110 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-08 22:18:12,135 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-08 22:18:12,299 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-08 22:18:12,302 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-08 22:18:12,375 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-08 22:18:12,375 - scrapy.core.engine - INFO - Spider opened
2018-05-08 22:18:12,477 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-08 22:18:13,603 - scrapy.core.scraper - ERROR - Error downloading <GET http://china.nba.com/news/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2018-05-08 22:18:13,708 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-08 22:18:13,709 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,
 'downloader/request_bytes': 216,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 8, 14, 18, 13, 708714),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 5, 8, 14, 18, 12, 477798)}
2018-05-08 22:18:13,709 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-08 22:18:14,036 - loggers - INFO -  Starting crawl ZhibobaSpider
2018-05-08 22:18:15,139 - loggers - INFO - this is a test
2018-05-08 22:18:15,140 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-08 22:18:15,140 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-08 22:18:15,167 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-08 22:18:15,168 - twisted - CRITICAL - Unhandled error in Deferred:
2018-05-08 22:18:15,168 - twisted - CRITICAL - 
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 96, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/ZhibobaSpider.py", line 16, in __init__
    super().__init__(config=self.config)
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 17, in __init__
    self.data = json2Dict(config_path)
  File "/Users/vincent/pyProjects/scrape/utils/json2Dict.py", line 45, in __init__
    self.start_url = json_data[START_URL]
TypeError: 'NoneType' object is not subscriptable
2018-05-08 22:24:39,639 - loggers - INFO - this is a test
2018-05-08 22:24:39,640 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-08 22:24:39,641 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-08 22:24:39,667 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-08 22:24:39,939 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-08 22:24:39,943 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-08 22:24:40,075 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-08 22:24:40,076 - scrapy.core.engine - INFO - Spider opened
2018-05-08 22:24:40,166 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-08 22:24:41,307 - scrapy.core.scraper - ERROR - Error downloading <GET http://china.nba.com/news/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.DNSLookupError: DNS lookup failed: address '500 internal server error\ninternal server error\nthe server encountered an internal error and was unable to complete your request.  either the server is overloaded or there is an error in the application.' not found: encoding with 'idna' codec failed (UnicodeError: label empty or too long).
2018-05-08 22:24:41,420 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-08 22:24:41,422 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,
 'downloader/request_bytes': 216,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 8, 14, 24, 41, 422053),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 5, 8, 14, 24, 40, 166502)}
2018-05-08 22:24:41,422 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-08 22:28:19,369 - loggers - INFO - this is a test
2018-05-08 22:28:19,370 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-08 22:28:19,370 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-08 22:28:19,416 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-08 22:28:19,417 - twisted - CRITICAL - Unhandled error in Deferred:
2018-05-08 22:28:19,418 - twisted - CRITICAL - 
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 96, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/ZhibobaSpider.py", line 16, in __init__
    super().__init__(config=self.config)
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 17, in __init__
    self.data = json2Dict(config_path)
  File "/Users/vincent/pyProjects/scrape/utils/json2Dict.py", line 45, in __init__
    self.start_url = json_data[START_URL]
TypeError: 'NoneType' object is not subscriptable
2018-05-09 10:27:40,282 - loggers - INFO -  Starting crawl SinaSpider
2018-05-09 10:27:41,887 - loggers - INFO - this is a test
2018-05-09 10:27:41,888 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:27:41,888 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:27:41,939 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:27:42,240 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:27:42,246 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:27:42,358 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:27:42,359 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:27:42,453 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:28:09,741 - scrapy.crawler - INFO - Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2018-05-09 10:28:09,744 - scrapy.core.engine - INFO - Closing spider (shutdown)
2018-05-09 10:29:21,587 - loggers - INFO -  Starting crawl SinaSpider
2018-05-09 10:29:23,029 - loggers - INFO - this is a test
2018-05-09 10:29:23,030 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:29:23,030 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:29:23,078 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:29:23,349 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:29:23,354 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:29:23,467 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:29:23,467 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:29:23,568 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:29:29,992 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/nba/> (referer: None)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 27, in parse_item
    print("列表："+link_list)
TypeError: must be str, not list
2018-05-09 10:29:30,011 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-09 10:29:30,012 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/request_bytes': 220,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 36374,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 9, 2, 29, 30, 11741),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2018, 5, 9, 2, 29, 23, 568579)}
2018-05-09 10:29:30,012 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-09 10:29:30,192 - loggers - INFO -  Starting crawl QqSpider
2018-05-09 10:29:31,281 - loggers - INFO - this is a test
2018-05-09 10:29:31,282 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:29:31,282 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:29:31,309 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:29:31,476 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:29:31,479 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:29:31,556 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:29:31,556 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:29:31,813 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:29:35,638 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.qq.com/nba/> (referer: None)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 27, in parse_item
    print("列表："+link_list)
TypeError: must be str, not list
2018-05-09 10:29:35,640 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-09 10:29:35,641 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/request_bytes': 215,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 35364,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 9, 2, 29, 35, 641392),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2018, 5, 9, 2, 29, 31, 813491)}
2018-05-09 10:29:35,641 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-09 10:29:36,199 - loggers - INFO -  Starting crawl ChinanbaSpider
2018-05-09 10:29:37,286 - loggers - INFO - this is a test
2018-05-09 10:29:37,286 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:29:37,287 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:29:37,310 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:29:37,478 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:29:37,481 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:29:37,553 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:29:37,553 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:29:37,630 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:29:43,008 - scrapy.core.scraper - ERROR - Spider error processing <GET http://china.nba.com/news/> (referer: None)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 27, in parse_item
    print("列表："+link_list)
TypeError: must be str, not list
2018-05-09 10:29:43,010 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-09 10:29:43,011 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/request_bytes': 216,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 22534,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 9, 2, 29, 43, 11549),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2018, 5, 9, 2, 29, 37, 630546)}
2018-05-09 10:29:43,012 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-09 10:29:43,241 - loggers - INFO -  Starting crawl ZhibobaSpider
2018-05-09 10:29:44,334 - loggers - INFO - this is a test
2018-05-09 10:29:44,334 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:29:44,334 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:29:44,359 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:29:44,521 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:29:44,524 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:29:44,601 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:29:44,601 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:29:44,817 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:29:53,610 - scrapy.core.scraper - ERROR - Spider error processing <GET https://news.zhibo8.cc/nba/> (referer: None)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 27, in parse_item
    print("列表："+link_list)
TypeError: must be str, not list
2018-05-09 10:29:53,613 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-09 10:29:53,613 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/request_bytes': 216,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 65527,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 9, 2, 29, 53, 613572),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2018, 5, 9, 2, 29, 44, 817502)}
2018-05-09 10:29:53,614 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-09 10:31:15,230 - loggers - INFO -  Starting crawl SinaSpider
2018-05-09 10:31:16,355 - loggers - INFO - this is a test
2018-05-09 10:31:16,356 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:31:16,356 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:31:16,382 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:31:16,551 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:31:16,554 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:31:16,627 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:31:16,627 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:31:16,746 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:31:21,194 - scrapy.crawler - INFO - Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2018-05-09 10:31:21,194 - scrapy.core.engine - INFO - Closing spider (shutdown)
2018-05-09 10:31:26,134 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/nba/> (referer: None)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 27, in parse_item
    print("列表："+link_list)
TypeError: must be str, not list
2018-05-09 10:31:26,138 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/request_bytes': 220,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 165477,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2018, 5, 9, 2, 31, 26, 137752),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 8,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2018, 5, 9, 2, 31, 16, 746387)}
2018-05-09 10:31:26,138 - scrapy.core.engine - INFO - Spider closed (shutdown)
2018-05-09 10:31:26,370 - loggers - INFO -  Starting crawl QqSpider
2018-05-09 10:31:27,506 - loggers - INFO - this is a test
2018-05-09 10:31:27,506 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:31:27,507 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:31:27,532 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:31:27,696 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:31:27,699 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:31:27,771 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:31:27,771 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:31:27,869 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:45:55,641 - loggers - INFO -  Starting crawl SinaSpider
2018-05-09 10:45:57,263 - loggers - INFO - this is a test
2018-05-09 10:45:57,263 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:45:57,264 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:45:57,316 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:45:57,624 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:45:57,631 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:45:57,738 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:45:57,739 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:45:57,838 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:46:20,531 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7387303.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2018-05-09 10:46:32,374 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-04/doc-ifzyqqiq6594583.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 37, in parse_detail
    print("结果："+response)
TypeError: must be str, not HtmlResponse
2018-05-09 10:46:32,382 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-04/doc-ifzyqqiq6388548.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2018-05-09 10:46:34,713 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7289729.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7289729.shtml took longer than 15.0 seconds..
2018-05-09 10:46:35,733 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-07/doc-ifzfkmth9904244.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 37, in parse_detail
    print("结果："+response)
TypeError: must be str, not HtmlResponse
2018-05-09 10:46:43,151 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6214882.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 37, in parse_detail
    print("结果："+response)
TypeError: must be str, not HtmlResponse
2018-05-09 10:46:44,197 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-05/doc-ifzfkmth9268970.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2018-05-09 10:46:50,472 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-06/doc-ihacuuvt8406922.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 37, in parse_detail
    print("结果："+response)
TypeError: must be str, not HtmlResponse
2018-05-09 10:46:51,750 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6249703.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6249703.shtml took longer than 15.0 seconds..
2018-05-09 10:46:55,374 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6571817.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 37, in parse_detail
    print("结果："+response)
TypeError: must be str, not HtmlResponse
2018-05-09 10:46:57,842 - scrapy.extensions.logstats - INFO - Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:46:59,869 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6587738.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 37, in parse_detail
    print("结果："+response)
TypeError: must be str, not HtmlResponse
2018-05-09 10:47:01,311 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6614562.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 37, in parse_detail
    print("结果："+response)
TypeError: must be str, not HtmlResponse
2018-05-09 10:47:10,622 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7209407.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 37, in parse_detail
    print("结果："+response)
TypeError: must be str, not HtmlResponse
2018-05-09 10:47:14,975 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6790922.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6790922.shtml took longer than 15.0 seconds..
2018-05-09 10:47:17,693 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7207051.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 37, in parse_detail
    print("结果："+response)
TypeError: must be str, not HtmlResponse
2018-05-09 10:47:19,410 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7036135.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7036135.shtml took longer than 15.0 seconds..
2018-05-09 10:47:21,689 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7224243.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 37, in parse_detail
    print("结果："+response)
TypeError: must be str, not HtmlResponse
2018-05-09 10:47:25,444 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6227036.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 37, in parse_detail
    print("结果："+response)
TypeError: must be str, not HtmlResponse
2018-05-09 10:47:30,006 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6219295.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 37, in parse_detail
    print("结果："+response)
TypeError: must be str, not HtmlResponse
2018-05-09 10:47:32,696 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7259169.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2018-05-09 10:47:36,147 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6187600.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 37, in parse_detail
    print("结果："+response)
TypeError: must be str, not HtmlResponse
2018-05-09 10:47:39,810 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6638318.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-09 10:47:46,319 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7175801.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-09 10:47:46,413 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7251748.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 37, in parse_detail
    print("结果："+response)
TypeError: must be str, not HtmlResponse
2018-05-09 10:47:47,284 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6661440.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 37, in parse_detail
    print("结果："+response)
TypeError: must be str, not HtmlResponse
2018-05-09 10:47:56,306 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7393937.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 37, in parse_detail
    print("结果："+response)
TypeError: must be str, not HtmlResponse
2018-05-09 10:47:57,842 - scrapy.extensions.logstats - INFO - Crawled 17 pages (at 11 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:48:00,644 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7307266.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 37, in parse_detail
    print("结果："+response)
TypeError: must be str, not HtmlResponse
2018-05-09 10:48:00,646 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-09 10:48:00,647 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 10,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 7,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 8536,
 'downloader/request_count': 28,
 'downloader/request_method_count/GET': 28,
 'downloader/response_bytes': 692486,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'dupefilter/filtered': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 9, 2, 48, 0, 646910),
 'log_count/DEBUG': 76,
 'log_count/ERROR': 27,
 'log_count/INFO': 9,
 'request_depth_max': 1,
 'response_received_count': 18,
 'scheduler/dequeued': 28,
 'scheduler/dequeued/memory': 28,
 'scheduler/enqueued': 28,
 'scheduler/enqueued/memory': 28,
 'spider_exceptions/TypeError': 17,
 'start_time': datetime.datetime(2018, 5, 9, 2, 45, 57, 838503)}
2018-05-09 10:48:00,647 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-09 10:48:01,263 - loggers - INFO -  Starting crawl QqSpider
2018-05-09 10:48:02,790 - loggers - INFO - this is a test
2018-05-09 10:48:02,790 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:48:02,791 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:48:02,858 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:48:03,091 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:48:03,095 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:48:03,191 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:48:03,191 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:48:04,278 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:48:20,423 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.qq.com/nba/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.qq.com/nba/ took longer than 15.0 seconds..
2018-05-09 10:48:20,534 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-09 10:48:20,535 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/request_bytes': 215,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 9, 2, 48, 20, 535325),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 5, 9, 2, 48, 4, 277890)}
2018-05-09 10:48:20,535 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-09 10:48:21,030 - loggers - INFO -  Starting crawl ChinanbaSpider
2018-05-09 10:48:22,213 - loggers - INFO - this is a test
2018-05-09 10:48:22,213 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:48:22,214 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:48:22,242 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:48:22,430 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:48:22,434 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:48:22,519 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:48:22,519 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:48:24,297 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:48:36,602 - loggers - INFO -  Starting crawl SinaSpider
2018-05-09 10:48:37,709 - loggers - INFO - this is a test
2018-05-09 10:48:37,710 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:48:37,710 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:48:37,734 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:48:37,894 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:48:37,897 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:48:37,969 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:48:37,969 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:48:38,069 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:48:54,198 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/nba/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/nba/ took longer than 15.0 seconds..
2018-05-09 10:48:54,302 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-09 10:48:54,303 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/request_bytes': 220,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 9, 2, 48, 54, 303410),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 5, 9, 2, 48, 38, 68902)}
2018-05-09 10:48:54,304 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-09 10:48:54,774 - loggers - INFO -  Starting crawl QqSpider
2018-05-09 10:48:55,847 - loggers - INFO - this is a test
2018-05-09 10:48:55,848 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:48:55,848 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:48:55,874 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:48:56,033 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:48:56,036 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:48:56,107 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:48:56,107 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:48:56,627 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:49:22,173 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.qq.com/a/20180509/014051.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.qq.com/a/20180509/014051.htm took longer than 15.0 seconds..
2018-05-09 10:49:40,202 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.qq.com/a/20180509/017577.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.qq.com/a/20180509/017577.htm took longer than 15.0 seconds..
2018-05-09 10:49:45,337 - scrapy.crawler - INFO - Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2018-05-09 10:49:45,338 - scrapy.core.engine - INFO - Closing spider (shutdown)
2018-05-09 10:50:52,274 - loggers - INFO -  Starting crawl SinaSpider
2018-05-09 10:50:53,379 - loggers - INFO - this is a test
2018-05-09 10:50:53,379 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:50:53,380 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:50:53,407 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:50:53,565 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:50:53,568 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:50:53,666 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:50:53,666 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:50:55,804 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:51:19,046 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7289729.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:51:21,841 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-04/doc-ifzyqqiq6388548.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:51:24,046 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-04/doc-ifzyqqiq6594583.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:51:35,642 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-07/doc-ifzfkmth9904244.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:51:42,746 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-05/doc-ifzfkmth9268970.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-05/doc-ifzfkmth9268970.shtml took longer than 15.0 seconds..
2018-05-09 10:51:43,767 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-06/doc-ihacuuvt8406922.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:51:48,061 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6587738.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-09 10:51:49,075 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6249703.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6249703.shtml took longer than 15.0 seconds..
2018-05-09 10:51:51,658 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6214882.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6214882.shtml took longer than 15.0 seconds..
2018-05-09 10:51:55,807 - scrapy.extensions.logstats - INFO - Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:51:57,317 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7036135.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:51:58,344 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6614562.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:52:04,182 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7209407.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:52:04,281 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6790922.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:52:04,291 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7207051.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:52:04,529 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7224243.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:52:10,509 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7259169.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:52:14,856 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6227036.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:52:24,535 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7550281.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2018-05-09 10:52:26,535 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6219295.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-09 10:52:28,124 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6661440.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:52:31,833 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6187600.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6187600.shtml took longer than 15.0 seconds..
2018-05-09 10:52:32,181 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6638318.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:52:36,624 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7175801.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:52:43,588 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7307266.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:52:45,427 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7251748.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:52:51,534 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7393937.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:52:54,163 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7533774.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 10:52:55,808 - scrapy.extensions.logstats - INFO - Crawled 21 pages (at 15 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:52:59,256 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7387303.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7387303.shtml took longer than 15.0 seconds..
2018-05-09 10:52:59,361 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-09 10:52:59,362 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 8,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 6,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'downloader/request_bytes': 8844,
 'downloader/request_count': 29,
 'downloader/request_method_count/GET': 29,
 'downloader/response_bytes': 642848,
 'downloader/response_count': 21,
 'downloader/response_status_count/200': 21,
 'dupefilter/filtered': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 9, 2, 52, 59, 362128),
 'log_count/DEBUG': 81,
 'log_count/ERROR': 28,
 'log_count/INFO': 9,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 29,
 'scheduler/dequeued/memory': 29,
 'scheduler/enqueued': 29,
 'scheduler/enqueued/memory': 29,
 'spider_exceptions/AttributeError': 20,
 'start_time': datetime.datetime(2018, 5, 9, 2, 50, 55, 804054)}
2018-05-09 10:52:59,363 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-09 10:52:59,959 - loggers - INFO -  Starting crawl QqSpider
2018-05-09 10:53:01,074 - loggers - INFO - this is a test
2018-05-09 10:53:01,074 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:53:01,075 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:53:01,099 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:53:01,258 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:53:01,261 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:53:01,332 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:53:01,332 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:53:01,416 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:53:17,545 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.qq.com/nba/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2018-05-09 10:53:17,652 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-09 10:53:17,653 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/request_bytes': 215,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 9, 2, 53, 17, 653626),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 5, 9, 2, 53, 1, 416658)}
2018-05-09 10:53:17,654 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-09 10:53:18,096 - loggers - INFO -  Starting crawl ChinanbaSpider
2018-05-09 10:53:19,426 - loggers - INFO - this is a test
2018-05-09 10:53:19,427 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:53:19,427 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:53:19,457 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:53:19,648 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:53:19,652 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:53:19,735 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:53:19,736 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:53:19,836 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:53:35,968 - scrapy.core.scraper - ERROR - Error downloading <GET http://china.nba.com/news/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://china.nba.com/news/ took longer than 15.0 seconds..
2018-05-09 10:53:36,076 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-09 10:53:36,077 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/request_bytes': 216,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 9, 2, 53, 36, 77110),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 5, 9, 2, 53, 19, 836621)}
2018-05-09 10:53:36,077 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-09 10:53:36,599 - loggers - INFO -  Starting crawl ZhibobaSpider
2018-05-09 10:53:37,703 - loggers - INFO - this is a test
2018-05-09 10:53:37,704 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:53:37,704 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:53:37,729 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:53:37,891 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:53:37,895 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:53:37,965 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:53:37,965 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:53:38,075 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 10:54:12,878 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-08/5af14fded63d6.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]
2018-05-09 10:54:23,389 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-08/5af14a798ed59.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://news.zhibo8.cc/nba/2018-05-08/5af14a798ed59.htm took longer than 15.0 seconds..
2018-05-09 10:54:25,069 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2018-05-08/5af16d06ce6d3.htm
2018-05-09 10:54:26,581 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-09/5af242a533382.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]
2018-05-09 10:54:31,634 - root - INFO - 已经爬过https://news.zhibo8.cc/nba/2018-05-08/5af19e9bca347.htm
2018-05-09 10:54:36,337 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-08/5af1793326a4c.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://news.zhibo8.cc/nba/2018-05-08/5af1793326a4c.htm took longer than 15.0 seconds..
2018-05-09 10:54:38,078 - scrapy.extensions.logstats - INFO - Crawled 6 pages (at 6 pages/min), scraped 5 items (at 5 items/min)
2018-05-09 10:54:49,928 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-09/5af23e5e86c99.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
scrapy.core.downloader.handlers.http11.TunnelError: Could not open CONNECT tunnel with proxy 183.131.74.253:8908 [{'status': 400, 'reason': b'Bad Request'}]
2018-05-09 10:54:53,147 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-09/5af24aa85a3f7.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]
2018-05-09 10:54:57,729 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-09/5af1cfc346a64.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2018-05-09 10:55:02,122 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-09/5af2407be8a3c.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://news.zhibo8.cc/nba/2018-05-09/5af2407be8a3c.htm took longer than 15.0 seconds..
2018-05-09 10:55:12,511 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-09/5af24d9a84073.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://news.zhibo8.cc/nba/2018-05-09/5af24d9a84073.htm took longer than 15.0 seconds..
2018-05-09 10:55:15,317 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-09/5af24cc4553b7.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://news.zhibo8.cc/nba/2018-05-09/5af24cc4553b7.htm took longer than 15.0 seconds..
2018-05-09 10:55:15,730 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-09/5af1d6b235257.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
scrapy.core.downloader.handlers.http11.TunnelError: Could not open CONNECT tunnel with proxy 117.190.90.20:8060 [b'<html>\r\n<head><title>400 Bad Req']
2018-05-09 10:55:20,703 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-09/5af258c980469.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2018-05-09 10:55:24,699 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-09/5af25bd9d0688.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://news.zhibo8.cc/nba/2018-05-09/5af25bd9d0688.htm took longer than 15.0 seconds..
2018-05-09 10:55:27,617 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-09/5af25d748883f.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2018-05-09 10:55:34,660 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-09/5af226a867213.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://news.zhibo8.cc/nba/2018-05-09/5af226a867213.htm took longer than 15.0 seconds..
2018-05-09 10:55:34,854 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-09/5af25e865562e.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-09 10:55:38,079 - scrapy.extensions.logstats - INFO - Crawled 12 pages (at 6 pages/min), scraped 11 items (at 6 items/min)
2018-05-09 10:55:43,825 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-09/5af25d8ddad40.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
scrapy.core.downloader.handlers.http11.TunnelError: Could not open CONNECT tunnel with proxy 101.1.27.82:80 [{'status': 400, 'reason': b'Bad Request'}]
2018-05-09 10:55:54,714 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-09/5af25c4250d61.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://news.zhibo8.cc/nba/2018-05-09/5af25c4250d61.htm took longer than 15.0 seconds..
2018-05-09 10:56:05,163 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-09/5af25ec775c5e.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2018-05-09 10:56:12,726 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/2018-05-09/5af2596e4bb8d.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://news.zhibo8.cc/nba/2018-05-09/5af2596e4bb8d.htm took longer than 15.0 seconds..
2018-05-09 10:56:12,831 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-09 10:56:12,832 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 20,
 'downloader/exception_type_count/scrapy.core.downloader.handlers.http11.TunnelError': 3,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 13,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 9515,
 'downloader/request_count': 34,
 'downloader/request_method_count/GET': 34,
 'downloader/response_bytes': 135185,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 9, 2, 56, 12, 832080),
 'item_scraped_count': 13,
 'log_count/DEBUG': 96,
 'log_count/ERROR': 20,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 14,
 'scheduler/dequeued': 34,
 'scheduler/dequeued/memory': 34,
 'scheduler/enqueued': 34,
 'scheduler/enqueued/memory': 34,
 'start_time': datetime.datetime(2018, 5, 9, 2, 53, 38, 75239)}
2018-05-09 10:56:12,832 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-09 10:58:55,958 - loggers - INFO -  Starting crawl SinaSpider
2018-05-09 10:58:57,399 - loggers - INFO - this is a test
2018-05-09 10:58:57,399 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:58:57,400 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:58:57,449 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:58:57,679 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:58:57,685 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:58:57,785 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:58:57,785 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:58:57,842 - scrapy.core.engine - INFO - Closing spider (shutdown)
2018-05-09 10:58:57,843 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2018, 5, 9, 2, 58, 57, 843462),
 'log_count/INFO': 6}
2018-05-09 10:58:57,843 - scrapy.core.engine - INFO - Spider closed (shutdown)
2018-05-09 10:58:57,844 - twisted - CRITICAL - Unhandled error in Deferred:
2018-05-09 10:58:57,844 - twisted - CRITICAL - 
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/crawler.py", line 74, in crawl
    yield self.engine.open_spider(self.spider, start_requests)
pymongo.errors.OperationFailure: Authentication failed.
2018-05-09 10:58:58,491 - loggers - INFO -  Starting crawl QqSpider
2018-05-09 10:58:59,606 - loggers - INFO - this is a test
2018-05-09 10:58:59,607 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:58:59,607 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:58:59,631 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:58:59,793 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:58:59,796 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:58:59,870 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:58:59,870 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:58:59,879 - scrapy.core.engine - INFO - Closing spider (shutdown)
2018-05-09 10:58:59,880 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2018, 5, 9, 2, 58, 59, 880029),
 'log_count/INFO': 6}
2018-05-09 10:58:59,880 - scrapy.core.engine - INFO - Spider closed (shutdown)
2018-05-09 10:58:59,880 - twisted - CRITICAL - Unhandled error in Deferred:
2018-05-09 10:58:59,881 - twisted - CRITICAL - 
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/crawler.py", line 74, in crawl
    yield self.engine.open_spider(self.spider, start_requests)
pymongo.errors.OperationFailure: Authentication failed.
2018-05-09 10:59:00,517 - loggers - INFO -  Starting crawl ChinanbaSpider
2018-05-09 10:59:01,592 - loggers - INFO - this is a test
2018-05-09 10:59:01,593 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:59:01,593 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:59:01,618 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:59:01,794 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:59:01,797 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:59:01,872 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:59:01,872 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:59:01,880 - scrapy.core.engine - INFO - Closing spider (shutdown)
2018-05-09 10:59:01,881 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2018, 5, 9, 2, 59, 1, 881059),
 'log_count/INFO': 6}
2018-05-09 10:59:01,881 - scrapy.core.engine - INFO - Spider closed (shutdown)
2018-05-09 10:59:01,882 - twisted - CRITICAL - Unhandled error in Deferred:
2018-05-09 10:59:01,882 - twisted - CRITICAL - 
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/crawler.py", line 74, in crawl
    yield self.engine.open_spider(self.spider, start_requests)
pymongo.errors.OperationFailure: Authentication failed.
2018-05-09 10:59:02,523 - loggers - INFO -  Starting crawl ZhibobaSpider
2018-05-09 10:59:03,618 - loggers - INFO - this is a test
2018-05-09 10:59:03,618 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 10:59:03,619 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 10:59:03,643 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 10:59:03,813 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 10:59:03,817 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 10:59:03,891 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 10:59:03,892 - scrapy.core.engine - INFO - Spider opened
2018-05-09 10:59:03,899 - scrapy.core.engine - INFO - Closing spider (shutdown)
2018-05-09 10:59:03,900 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2018, 5, 9, 2, 59, 3, 900204),
 'log_count/INFO': 6}
2018-05-09 10:59:03,900 - scrapy.core.engine - INFO - Spider closed (shutdown)
2018-05-09 10:59:03,900 - twisted - CRITICAL - Unhandled error in Deferred:
2018-05-09 10:59:03,901 - twisted - CRITICAL - 
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/crawler.py", line 74, in crawl
    yield self.engine.open_spider(self.spider, start_requests)
pymongo.errors.OperationFailure: Authentication failed.
2018-05-09 11:06:52,422 - loggers - INFO -  Starting crawl SinaSpider
2018-05-09 11:06:53,930 - loggers - INFO - this is a test
2018-05-09 11:06:53,931 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 11:06:53,931 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 11:06:53,973 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 11:06:54,247 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 11:06:54,251 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 11:06:54,359 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 11:06:54,359 - scrapy.core.engine - INFO - Spider opened
2018-05-09 11:06:54,794 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 11:07:21,892 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-04/doc-ifzyqqiq6388548.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-09 11:07:26,351 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-04/doc-ifzyqqiq6594583.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 11:07:29,985 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7289729.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2018-05-09 11:07:36,462 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6249703.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-09 11:07:41,292 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-05/doc-ifzfkmth9268970.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-05/doc-ifzfkmth9268970.shtml took longer than 15.0 seconds..
2018-05-09 11:07:44,409 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-07/doc-ifzfkmth9904244.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-07/doc-ifzfkmth9904244.shtml took longer than 15.0 seconds..
2018-05-09 11:07:45,560 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6214882.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 11:07:53,877 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-06/doc-ihacuuvt8406922.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 11:07:54,950 - scrapy.extensions.logstats - INFO - Crawled 5 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 11:07:54,955 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6614562.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 11:07:58,882 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7036135.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 11:08:04,489 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6790922.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6790922.shtml took longer than 15.0 seconds..
2018-05-09 11:08:12,044 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7259169.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 11:08:13,059 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7209407.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7209407.shtml took longer than 15.0 seconds..
2018-05-09 11:08:13,161 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7550281.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-09 11:08:15,496 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7207051.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7207051.shtml took longer than 15.0 seconds..
2018-05-09 11:08:19,599 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7224243.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7224243.shtml took longer than 15.0 seconds..
2018-05-09 11:08:30,729 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6661440.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 11:08:32,981 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6227036.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6227036.shtml took longer than 15.0 seconds..
2018-05-09 11:08:33,220 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7175801.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 11:08:33,806 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7251748.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 11:08:35,870 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6187600.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy6187600.shtml took longer than 15.0 seconds..
2018-05-09 11:08:51,076 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7307266.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7307266.shtml took longer than 15.0 seconds..
2018-05-09 11:08:52,942 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7393937.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 11:08:53,650 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7669605.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 11:08:54,796 - scrapy.extensions.logstats - INFO - Crawled 12 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 11:08:55,440 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7387303.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 11:09:02,546 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7688654.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 11:09:02,809 - scrapy.core.scraper - ERROR - Spider error processing <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7533774.shtml> (referer: http://sports.sina.com.cn/nba/)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/vincent/pyProjects/scrape/scrape_news/spiders/CommonSpider.py", line 42, in parse_detail
    print(response.xpath(xpath.get_title()).extract_first().strip())
AttributeError: 'NoneType' object has no attribute 'strip'
2018-05-09 11:09:10,465 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.sina.com.cn/basketball/nba/2018-05-09/doc-ihaichqy7681456.shtml>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2018-05-09 11:09:10,570 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-09 11:09:10,572 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 14,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 11,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 9152,
 'downloader/request_count': 30,
 'downloader/request_method_count/GET': 30,
 'downloader/response_bytes': 420232,
 'downloader/response_count': 16,
 'downloader/response_status_count/200': 15,
 'downloader/response_status_count/403': 1,
 'dupefilter/filtered': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 9, 3, 9, 10, 571482),
 'log_count/DEBUG': 79,
 'log_count/ERROR': 28,
 'log_count/INFO': 9,
 'request_depth_max': 1,
 'response_received_count': 15,
 'scheduler/dequeued': 30,
 'scheduler/dequeued/memory': 30,
 'scheduler/enqueued': 30,
 'scheduler/enqueued/memory': 30,
 'spider_exceptions/AttributeError': 14,
 'start_time': datetime.datetime(2018, 5, 9, 3, 6, 54, 794868)}
2018-05-09 11:09:10,572 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-09 11:09:11,208 - loggers - INFO -  Starting crawl QqSpider
2018-05-09 11:09:12,821 - loggers - INFO - this is a test
2018-05-09 11:09:12,822 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 11:09:12,823 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 11:09:12,865 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 11:09:13,124 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 11:09:13,129 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 11:09:13,223 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 11:09:13,223 - scrapy.core.engine - INFO - Spider opened
2018-05-09 11:09:13,334 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 11:09:29,117 - root - INFO - 已经爬过http://sports.qq.com/a/20180509/005559.htm
2018-05-09 11:09:34,353 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.qq.com/a/20180508/022042.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-09 11:09:51,333 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.qq.com/a/20180508/026702.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2018-05-09 11:10:00,130 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.qq.com/a/20180508/038093.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2018-05-09 11:10:13,338 - scrapy.extensions.logstats - INFO - Crawled 6 pages (at 6 pages/min), scraped 5 items (at 5 items/min)
2018-05-09 11:10:14,879 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.qq.com/a/20180509/019847.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.qq.com/a/20180509/019847.htm took longer than 15.0 seconds..
2018-05-09 11:10:19,191 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.qq.com/a/20180509/019480.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.qq.com/a/20180509/019480.htm took longer than 15.0 seconds..
2018-05-09 11:10:26,070 - root - INFO - 已经爬过http://sports.qq.com/a/20180509/014012.htm
2018-05-09 11:10:31,789 - scrapy.core.scraper - ERROR - Error downloading <GET http://sports.qq.com/a/20180509/017577.htm>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting http://sports.qq.com/a/20180509/017577.htm took longer than 15.0 seconds..
2018-05-09 11:10:31,898 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-09 11:10:31,900 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 6,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 5,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 3699,
 'downloader/request_count': 14,
 'downloader/request_method_count/GET': 14,
 'downloader/response_bytes': 261980,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 7,
 'downloader/response_status_count/403': 1,
 'dupefilter/filtered': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 9, 3, 10, 31, 899901),
 'item_scraped_count': 6,
 'log_count/DEBUG': 45,
 'log_count/ERROR': 6,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 7,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 5, 9, 3, 9, 13, 333895)}
2018-05-09 11:10:31,900 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-09 11:10:32,500 - loggers - INFO -  Starting crawl ChinanbaSpider
2018-05-09 11:10:34,226 - loggers - INFO - this is a test
2018-05-09 11:10:34,226 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 11:10:34,227 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 11:10:34,280 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 11:10:34,594 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 11:10:34,600 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 11:10:34,707 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 11:10:34,707 - scrapy.core.engine - INFO - Spider opened
2018-05-09 11:10:34,805 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 11:10:36,213 - scrapy.core.scraper - ERROR - Error downloading <GET http://china.nba.com/news/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-05-09 11:10:36,323 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-09 11:10:36,325 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 216,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 9, 3, 10, 36, 324557),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 5, 9, 3, 10, 34, 805756)}
2018-05-09 11:10:36,325 - scrapy.core.engine - INFO - Spider closed (finished)
2018-05-09 11:10:36,854 - loggers - INFO -  Starting crawl ZhibobaSpider
2018-05-09 11:10:37,916 - loggers - INFO - this is a test
2018-05-09 11:10:37,916 - scrapy.utils.log - INFO - Scrapy 1.3.3 started (bot: scrape_news)
2018-05-09 11:10:37,917 - scrapy.utils.log - INFO - Overridden settings: {'BOT_NAME': 'scrape_news', 'DOWNLOAD_DELAY': 3, 'DOWNLOAD_TIMEOUT': 15, 'NEWSPIDER_MODULE': 'scrape_news.spiders', 'RETRY_ENABLED': False, 'SPIDER_MODULES': ['scrape_news.spiders']}
2018-05-09 11:10:37,941 - scrapy.middleware - INFO - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-05-09 11:10:38,119 - scrapy.middleware - INFO - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrape_news.middlewares.CustomDownloaderMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-05-09 11:10:38,122 - scrapy.middleware - INFO - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-05-09 11:10:38,197 - scrapy.middleware - INFO - Enabled item pipelines:
['scrape_news.pipelines.ScrapeNewsPipeline']
2018-05-09 11:10:38,197 - scrapy.core.engine - INFO - Spider opened
2018-05-09 11:10:38,748 - scrapy.extensions.logstats - INFO - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-05-09 11:10:54,884 - scrapy.core.scraper - ERROR - Error downloading <GET https://news.zhibo8.cc/nba/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.6/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 306, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://news.zhibo8.cc/nba/ took longer than 15.0 seconds..
2018-05-09 11:10:54,995 - scrapy.core.engine - INFO - Closing spider (finished)
2018-05-09 11:10:54,996 - scrapy.statscollectors - INFO - Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/request_bytes': 216,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 5, 9, 3, 10, 54, 996001),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 5, 9, 3, 10, 38, 748139)}
2018-05-09 11:10:54,996 - scrapy.core.engine - INFO - Spider closed (finished)
